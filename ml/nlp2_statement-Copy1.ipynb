{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dPfVKv2adDm"
   },
   "source": [
    "# Машинное обучение, DS-поток\n",
    "## Задание 1.11\n",
    "\n",
    "\n",
    "**Правила:**\n",
    "\n",
    "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ad21_bot`.\n",
    "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Прислать нужно ноутбук в формате `ipynb`.\n",
    "* Решения, размещенные на каких-либо интернет-ресурсах не принимаются. Публикация решения может быть приравнена к предоставлении возможности списать.\n",
    "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него.\n",
    "\n",
    "---\n",
    "\n",
    "## Генерация текстов с использованием RNN.\n",
    "\n",
    "**Части задания.** \n",
    "\n",
    "* Написание модели, лосса и кода для обучения (7 баллов)\n",
    "\n",
    "* Реализация жадного алгоритма генерации (3 балла)\n",
    "\n",
    "* Одно из:\n",
    "\n",
    "  * Реализация Top k sampling (2 балла)\n",
    "\n",
    "  * Реализация Beam-search (6 баллов)\n",
    "\n",
    "\n",
    "В данном задании вы будете генерировать тексты на основании высказываний [Ницше](https://ru.wikipedia.org/wiki/Ницше,_Фридрих) при помощи рекуррентных сетей. \n",
    "Модель будет основываться на токенах — словах или более продвинутых способах работы с текстом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Xt4DweWs4RO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qED98FUNs_ej"
   },
   "source": [
    "Скачиваем данные. Корпус состоит из высказываний Ницше, разделеных переносом строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxtkObdPtEd1",
    "outputId": "7aeecbde-94a0-47fc-d4d8-862328d8372e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n",
      "\"id\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QefWGfvcn0CXzmd42ySFTkwd1shHTUA3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ9QYAI7XFHX",
    "outputId": "76682721-da61-477b-c070-0c335c99f74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to Truth, have been unskilled and unseemly methods for\n",
      "winning a woman? Certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n"
     ]
    }
   ],
   "source": [
    "!head nietzsche.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEWI-vHktPSZ"
   },
   "source": [
    "Посмотрим сколько уникальных слов есть в корпусе.\n",
    "Также поставим каждому слову (токену) в соответствие число, чтобы далее работать с числами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qr1njwhtHzz",
    "outputId": "cafa72f1-3abb-44c9-e4fe-800f27b7c8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 11393\n"
     ]
    }
   ],
   "source": [
    "with io.open('nietzsche.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = list(set(tokenizer.tokenize(text)))\n",
    "print('total tokens:', len(tokens))\n",
    "\n",
    "token_indices = {c: i for i, c in enumerate(tokens)}\n",
    "indices_token = {i: c for i, c in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnbPfiSn-e4-"
   },
   "source": [
    "11393 уникальных слов — это относительно много, так как придется на каждом шаге решать задачу классификации на 11393 класса. А брать софтмакс от вектора такой большой размерности затруднительно. Поэтому имеется несколько вариантов: простой (1), очень простой (2) и современный (3) .\n",
    "\n",
    "1. Отбросить редко встречающиеся слова (токены) и заменить их все на токен \\<UNK\\>. \n",
    "\n",
    "2. Использовать в качестве токенов символы, а не слова. Такую модель мы использовали на семинаре для генерации имён.\n",
    "Этот подход имеет явный недостаток — модели, обучаемой на отдельных символах, гораздо сложнее выучить соотношения между буквами в каждом отдельном слове. Из-за этого может возникнуть ситуация, при которой значительная часть слов, сгенерированных моделью, не будут являться словами языка, на котором написаны входные тексты.\n",
    "\n",
    "3. Использовать [Byte Pair Encoding](https://arxiv.org/pdf/1508.07909.pdf). [Одна из опен-сорсных реализаций](https://github.com/VKCOM/YouTokenToMe) от ребят из VK.\n",
    "\n",
    "Существуют также подходы для аппроксимации softmax, такие как иерархический софтмакс и negative sampling.\n",
    "\n",
    "Разберём, как работает BPE на примере строки `aaabdaaabac`.\n",
    "Пара `aa` встречается чаще всего, поэтому она будет заменяется символом `Z`, который не используется в данных, получается строка `ZabdZabac`.\n",
    "Далее процесс повторяется для пары `ab`, которая заменяется на символ `Y`. Теперь строка имеет вид `ZYdZYac`, причем единственная оставшаяся пара исходных символов встречается только один раз. На этом кодировку можно остановить или же продолжать рекурсивно заменив `ZY` на `X`: `XdXac`. Полученную строку нельзя продолжать сжимать, поскольку не существует пары символов, встречающихся более одного раза. Для декодирования нужно выполнить замены в обратном порядке.\n",
    "\n",
    "Если же мы будем проделывать эту процедуру не над маленькой строкой, а над большим датасетом, то мы найдем часто встречающиеся n-gram-ы из символов в датасете. Полученные замены будем использовать как новые токены и представлять каждое слово как последовательность таких токенов.\n",
    "В данном случае получится, что частые слова скорей всего будут представлены лишь одним токеном, равным самому слову. А редкие слова разобъются на последовательность токенов, являющихся частыми.\n",
    "\n",
    "\n",
    "![](https://docs.google.com/uc?export=download&id=1bD-VA4EWfmMsqFT4ufIhCMDh2fKlP_rC)\n",
    "\n",
    "Метод BPE является некоторой золотой серединой между character-based и word-based подходами.\n",
    "[`YouTokenToMe`](https://github.com/VKCOM/YouTokenToMe) — одна из опен-сорсных реализаций от разработчиков из VK. \n",
    "Также можно посмотреть [статью](https://arxiv.org/abs/1910.13267) студента ФИВТа с конференции ACL 2020 по bpe-dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E41erLUtM6ts"
   },
   "source": [
    "Ставим библиотеку, речь о которой шла выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"vs_buildtools.exe\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n"
     ]
    }
   ],
   "source": [
    "! vs_buildtools.exe --norestart --passive --downloadThenInstall --includeRecommended --add Microsoft.VisualStudio.Workload.NativeDesktop --add Microsoft.VisualStudio.Workload.VCTools --add Microsoft.VisualStudio.Workload.MSBuildTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByZPyRZevQnq",
    "outputId": "1ea5a449-6154-45ef-b876-75e9e97ed343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtokentome\n",
      "  Using cached youtokentome-1.0.6.tar.gz (86 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Click>=7.0 in c:\\users\\poloz\\anaconda3\\envs\\cvxpy\\lib\\site-packages (from youtokentome) (8.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\poloz\\anaconda3\\envs\\cvxpy\\lib\\site-packages (from Click>=7.0->youtokentome) (0.4.4)\n",
      "Building wheels for collected packages: youtokentome\n",
      "  Building wheel for youtokentome (setup.py): started\n",
      "  Building wheel for youtokentome (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for youtokentome\n",
      "Failed to build youtokentome\n",
      "Installing collected packages: youtokentome\n",
      "  Running setup.py install for youtokentome: started\n",
      "  Running setup.py install for youtokentome: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [22 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\youtokentome.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\yttm_cli.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\__init__.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  running build_ext\n",
      "  building '_youtokentome_cython' extension\n",
      "  creating build\\temp.win-amd64-cpython-39\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\youtokentome\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\youtokentome\\cpp\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Iyoutokentome/cpp -IC:\\Users\\poloz\\anaconda3\\envs\\cvxpy\\include -IC:\\Users\\poloz\\anaconda3\\envs\\cvxpy\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpyoutokentome/cpp/bpe.cpp /Fobuild\\temp.win-amd64-cpython-39\\Release\\youtokentome/cpp/bpe.obj -std=c++11 -pthread -O3\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-std=c++11\"\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-pthread\"\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-O3\"\n",
      "  bpe.cpp\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\cstddef(12): fatal error C1083: Не удается открыть файл включение: stddef.h: No such file or directory,\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for youtokentome\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for youtokentome did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [24 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\poloz\\anaconda3\\envs\\cvxpy\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\youtokentome.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\yttm_cli.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  copying youtokentome\\__init__.py -> build\\lib.win-amd64-cpython-39\\youtokentome\n",
      "  running build_ext\n",
      "  building '_youtokentome_cython' extension\n",
      "  creating build\\temp.win-amd64-cpython-39\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\youtokentome\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\youtokentome\\cpp\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Iyoutokentome/cpp -IC:\\Users\\poloz\\anaconda3\\envs\\cvxpy\\include -IC:\\Users\\poloz\\anaconda3\\envs\\cvxpy\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpyoutokentome/cpp/bpe.cpp /Fobuild\\temp.win-amd64-cpython-39\\Release\\youtokentome/cpp/bpe.obj -std=c++11 -pthread -O3\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-std=c++11\"\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-pthread\"\n",
      "  cl: командная строка warning D9002: пропуск неизвестного параметра \"-O3\"\n",
      "  bpe.cpp\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\cstddef(12): fatal error C1083: Не удается открыть файл включение: stddef.h: No such file or directory,\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "youtokentome\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "! pip install youtokentome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1Lh_7mjM9fz"
   },
   "source": [
    "Создаем BPE-модель, которую обучим на наших токенах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xDWt7rAHvxZa"
   },
   "outputs": [],
   "source": [
    "import youtokentome as yttm\n",
    "\n",
    "# Зададим желаемый размер словаря после BPE.\n",
    "# BPE перестанет объединять символы и токены в тот момент, \n",
    "# когда текущее количество токенов >= vocab_size\n",
    "vocab_size = 1000\n",
    "\n",
    "def get_bpe(tokens, vocab_size=vocab_size):\n",
    "    \"\"\" \n",
    "    Возвращает токенизатор BPE, обученный на токенах.\n",
    "    Параметры.\n",
    "    1) tokens - токены,\n",
    "    2) vocab_size - количество уникальных токенов в итоговом словаре.\n",
    "    \"\"\"\n",
    "\n",
    "    with open('tmp.json', 'w', encoding='utf-8') as file_:\n",
    "        for token in tokens:\n",
    "            print(token, file=file_)\n",
    "\n",
    "    yttm.BPE.train('tmp.json', vocab_size=vocab_size, model=\"bpe.model\")\n",
    "    os.remove('tmp.json')\n",
    "\n",
    "    return yttm.BPE(model=\"bpe.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l5jhKmf2wSHA"
   },
   "outputs": [],
   "source": [
    "bpe = get_bpe(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.subword_to_id('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pu0MlfyNCZD"
   },
   "source": [
    "Пример работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PBmpg7zwf66",
    "outputId": "bd329393-30ed-47ff-dce6-37f52d6917e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  supposing that truth is a woman--what then? is there not ground \n",
      "\n",
      "BPE decoded:  <BOS> ▁sup pos ing ▁th at ▁truth ▁is ▁a ▁w oman --what ▁th en ? ▁is ▁there ▁not ▁gr ound <EOS> \n",
      "\n",
      "BPE encoded:  [2, 597, 268, 69, 153, 79, 871, 772, 86, 101, 595, 712, 153, 64, 55, 772, 612, 763, 228, 267, 3]\n"
     ]
    }
   ],
   "source": [
    "example = text.split('\\n')[3]\n",
    "\n",
    "print('Original: ', example, '\\n')\n",
    "print('BPE decoded: ', *bpe.encode(\n",
    "    example, output_type=yttm.OutputType.SUBWORD, bos=True, eos=True\n",
    "    ), '\\n', sep=' ')\n",
    "print('BPE encoded: ', bpe.encode(\n",
    "    example, output_type=yttm.OutputType.ID, bos=True, eos=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6e9oGs4adDo"
   },
   "source": [
    "Напишите нейронную сеть, использующую RNN, которая должна обучиться на *каждом шаге* предсказывать следующий токен. Для этого вам понадобится к каждому скрытому состоянию RNN применить некоторую классификационную нейронную сеть. На каждом этапе времени такая сеть будет пытаться предсказать следующий токен. На каждом этапе времени должно получиться свое предсказание вероятности для следующего токена. После этого необходимо усреднить лосс по полученным предсказаниям.\n",
    "\n",
    "**Замечания** <br>\n",
    "1). Проследите, что вы действительно предсказываете следующий символ, а не текущий. Для этого можно поступить следующим образом.\n",
    "\n",
    "Пусть на вход приходит последовательность $x_1, x_2, ..., x_{n}$. Требуется на каждом этапе времени $t$ по символам $x_1, ..., x_{t - 1}$ предсказывать символ $x_t$. Для этого на вход нужно подать последовательность \\<BOS\\>, $x_1, x_2..., x_{n}$ и в качестве таргетов взять последовательность $x_1, x_2, ..., x_{n}, $ \\<EOS\\>. В данном случае \\<BOS\\> (begin of sentence), \\<EOS\\>(end of sentence) — специальные символы начала и конца предложения (любые новые символы, которых нет в словаре). \n",
    "\n",
    "2). При обучении вы подаете на вход истинные токены и предсказываете следующий токен, но при тестировании истинных токенов нет. Поэтому при тестировании стоит поступить следующим образом.\n",
    "* Подать на вход \\<BOS\\>, применить рекуррентную ячейку и предсказать вероятности быть следующим символом для каждого токена из словаря. \n",
    "* Найти токен с максимальной вероятностью. Обозначим его $x_1$. \n",
    "* Подать символ $x_1$ на вход рекуррентной ячейке и предсказать вероятности для следующих символов. \n",
    "* И так далее. Генерация продолжается до тех пор, пока не был предсказан токен \\<EOS\\> или мы не достигли нужной `length_to_predict` длины сгенерированного текста, где `length_to_predict` — число, заданное заранее.\n",
    "\n",
    "Заметим, что при наличии начальной последовательности $x_1, .., x_k$, которую нужно продолжить, сначала на вход нужно подавать истинные символы, а затем, когда истинные закончатся, подавать предсказанные символы. То есть в этом случае мы подаем \\<BOS\\>, $x_1, .., x_k$ на вход и уже потом начинаем генерацию.\n",
    "\n",
    "На рисунке ниже можно увидеть пример такой сети. При обучении на вход подаются истинные токены. На этапе времени $t - 1$ предсказываются вероятности $\\widehat{y}_{i}$ быть следующим ($i$-ым) токеном для всех токенов из словаря. \n",
    "В качестве лосса в данном случае рассматривается функция $\\frac{1}{n} \\sum\\limits_{i=1}^{n} H(y_i, \\widehat{y}_i)$, где $H(\\mathsf{P}, \\mathsf{Q})$ — кросс-энтропия, а $y^i$ — вырожденное распределение, соответствующее истинному индексу $i$-ого токена.\n",
    "\n",
    "На этапе применения на вход рекуррентной ячейке подается не настоящий токен, а тот токен, который имел наибольшую вероятность быть следующим на предыдущем шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMgb5Xl2adDp"
   },
   "source": [
    "![img](https://i.ibb.co/BnftCLh/Screenshot-2019-05-03-at-21-45-01.png)\n",
    "\n",
    "![imh](https://i.ibb.co/rwHyYsK/Screenshot-2019-05-03-at-21-44-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F8odJpoCXK5"
   },
   "source": [
    "Получим таргеты для обучения. Для этого к каждой строке, т.е. последовательности bpe-токенов добавим токен '\\<EOS\\>'. Также, сделаем так, чтобы все строки были одной и той же длины, добавив паддинг из символов '\\<PAD\\>'.\n",
    "\n",
    "Таким образом, строка $x_1, x_2, ..., x_{8}$ должна преобразиться в $x_1, x_2, ..., x_{8}$, \\<EOS\\>, \\<PAD\\>, \\<PAD\\>, \\<PAD\\> в случае если мы хотим, чтобы длина всех строк была 12.\n",
    "\n",
    "После данных преобразований переведите полученные строки в последовательность чисел, с которыми уже далее будет работать модель. Для этого можно использовать метод `bpe.subword_to_id`\n",
    "\n",
    "Для этого сначала найдем максимальную длину строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r5MymVXTD-lB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длина строки: 40\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "\n",
    "for string in text.split('\\n'):\n",
    "    bpe_string = bpe.encode(string, output_type=yttm.OutputType.SUBWORD, bos=False, eos=False)\n",
    "    if len(bpe_string) > 0:\n",
    "        length.append(len(bpe_string))\n",
    "print(\"Максимальная длина строки:\", np.max(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGjj7L6OF24F"
   },
   "source": [
    "Посчитаем таргеты. Учитывайте, что так как мы добавляем символ \\<EOS\\>, то полученная длина строки должна быть на 1 больше.\n",
    "\n",
    "*Замечание* \n",
    "\n",
    "После разбиения текста `text` на строки стоит проверить, что все строки имеют ненулевую длину. Строки нулевой длины нужно выкинуть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NeXRQPrTD7kK"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = np.max(length) + 1\n",
    "\n",
    "target_strings = []\n",
    "\n",
    "for string in text.split('\\n'):\n",
    "    bpe_string = bpe.encode(string, output_type=yttm.OutputType.SUBWORD, bos=False, eos=False)\n",
    "    if len(bpe_string) > 0:\n",
    "        bpe_string.append('<EOS>')\n",
    "        while len(bpe_string) < MAX_LENGTH:\n",
    "            bpe_string.append('<PAD>')\n",
    "        target_strings.append(bpe_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁pre', 'f', 'ace', '<EOS>', '<PAD>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_strings[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!' '\"' \"'\" \"'s\" '(' ')' ',' '-' '--' '--a' '--and' '--as' '--b' '--but'\n",
      " '--d' '--for' '--h' '--he' '--i' '--in' '--is' '--it' '--m' '--n'\n",
      " '--namely' '--or' '--s' '--th' '--that' '--the' '--they' '--this' '--to'\n",
      " '--w' '--we' '--wh' '--what' '--who' '-b' '-c' '-d' '-f' '-g' '-h' '-l'\n",
      " '-like' '-m' '-mor' '-n' '-p' '-r' '-re' '-s' '-t' '-w' '.' '0' '1' '2'\n",
      " '3' '4' '5' '6' '7' '8' '9' ':' ';' '<EOS>' '<PAD>' '=' '?' '[' ']' '_'\n",
      " 'a' 'ab' 'ability' 'able' 'ably' 'ac' 'ace' 'ach' 'ack' 'acr' 'acy' 'ad'\n",
      " 'ade' 'adic' 'ads' 'af' 'ag' 'age' 'ages' 'aid' 'ail' 'ain' 'ained'\n",
      " 'aining' 'ains' 'air' 'ak' 'ake' 'aking' 'al' 'ality' 'aliz' 'all' 'ally'\n",
      " 'als' 'alth' 'am' 'ame' 'amely' 'ament' 'an' 'anc' 'ance' 'ances' 'and'\n",
      " 'ands' 'ane' 'ang' 'ank' 'ann' 'ans' 'ant' 'ants' 'ap' 'aph' 'app' 'aps'\n",
      " 'ar' 'ard' 'are' 'arily' 'arm' 'arn' 'ars' 'art' 'ary' 'as' 'ase' 'ash'\n",
      " 'ason' 'ass' 'ast' 'aste' 'at' 'ate' 'ated' 'ately' 'ates' 'ath' 'ather'\n",
      " 'ati' 'atic' 'ating' 'ation' 'ations' 'atis' 'atisf' 'ative' 'ator'\n",
      " 'ators' 'atter' 'atur' 'atural' 'ature' 'au' 'augh' 'aus' 'av' 'ave'\n",
      " 'avour' 'aw' 'ax' 'ay' 'ays' 'az' 'b' 'bed' 'ber' 'ble' 'br' 'c' 'cc'\n",
      " 'ce' 'ceiv' 'cep' 'ception' 'ch' 'che' 'ci' 'con' 'cr' 'crib' 'cul' 'd'\n",
      " 'der' 'e' 'ear' 'ec' 'ecess' 'ect' 'ection' 'ed' 'edi' 'educ' 'el'\n",
      " 'eless' 'elf' 'ell' 'elop' 'els' 'ely' 'em' 'emb' 'ement' 'emp' 'en'\n",
      " 'enc' 'ence' 'ences' 'ency' 'end' 'ended' 'ending' 'ends' 'ene' 'ened'\n",
      " 'ener' 'eness' 'eng' 'ening' 'ens' 'ent' 'enti' 'ention' 'ently' 'ents'\n",
      " 'ep' 'er' 'ere' 'ered' 'ering' 'ern' 'ers' 'erst' 'ertain' 'erv' 'ery'\n",
      " 'es' 'ess' 'est' 'esti' 'estion' 'et' 'ev' 'ever' 'ex' 'f' 'fer' 'ff'\n",
      " 'fic' 'for' 'form' 'ful' 'fully' 'fulness' 'g' 'ge' 'ged' 'gh' 'gl' 'go'\n",
      " 'gr' 'h' 'hip' 'i' 'ia' 'ial' 'ian' 'ians' 'iation' 'ib' 'ibility' 'ible'\n",
      " 'ic' 'ical' 'ice' 'ich' 'ick' 'ict' 'id' 'idd' 'ide' 'ider' 'ides'\n",
      " 'idity' 'ie' 'ied' 'ief' 'ient' 'ier' 'ies' 'iest' 'iet' 'if' 'ife'\n",
      " 'ific' 'ification' 'ified' 'ig' 'igh' 'ight' 'ights' 'ign' 'il' 'ild'\n",
      " 'ile' 'ility' 'ill' 'ilosoph' 'ilt' 'ily' 'im' 'in' 'ination' 'inc' 'ind'\n",
      " 'ine' 'ined' 'ines' 'iness' 'ing' 'ing-' 'ingly' 'ings' 'ink' 'ins'\n",
      " 'inter' 'ion' 'ions' 'ior' 'ious' 'iously' 'iousness' 'ip' 'ir' 'ire'\n",
      " 'irst' 'is' 'ise' 'ised' 'ises' 'ish' 'ished' 'ishes' 'ishing' 'ising'\n",
      " 'ism' 'ison' 'iss' 'ist' 'isti' 'istic' 'ists' 'it' 'itable' 'itation'\n",
      " 'ite' 'ited' 'ith' 'ither' 'iti' 'itic' 'ities' 'iting' 'ition' 'its'\n",
      " 'itself' 'ity' 'iv' 'ive' 'iver' 'ix' 'iz' 'ize' 'ized' 'izing' 'j' 'jec'\n",
      " 'k' 'ke' 'ked' 'king' 'ks' 'l' 'ld' 'le' 'leas' 'lec' 'led' 'leness'\n",
      " 'les' 'less' 'lessness' 'let' 'li' 'lig' 'light' 'like' 'lim' 'ling'\n",
      " 'lus' 'ly' 'm' 'man' 'me' 'ment' 'ments' 'min' 'ms' 'n' 'ne' 'ner' 'ness'\n",
      " 'ning' 'now' 'o' 'ob' 'oble' 'oc' 'ocr' 'od' 'oduc' 'oes' 'of' 'og' 'ogn'\n",
      " 'oin' 'ok' 'ol' 'old' 'oll' 'olog' 'ology' 'olu' 'olution' 'om' 'oman'\n",
      " 'ome' 'omin' 'omp' 'on' 'ond' 'one' 'ong' 'ons' 'ont' 'oo' 'ood' 'ook'\n",
      " 'ool' 'oom' 'op' 'ope' 'oph' 'or' 'ord' 'ore' 'orm' 'orn' 'ors' 'ort'\n",
      " 'ortun' 'ory' 'os' 'ose' 'oss' 'ost' 'ot' 'oth' 'ou' 'oub' 'ouch' 'ough'\n",
      " 'ought' 'oun' 'ound' 'ounds' 'our' 'ours' 'ous' 'ously' 'out' 'ov' 'ove'\n",
      " 'oved' 'over' 'ow' 'owed' 'ower' 'owing' 'own' 'ows' 'ox' 'oy' 'p' 'par'\n",
      " 'pec' 'pect' 'per' 'ph' 'pir' 'pl' 'plic' 'pos' 'position' 'pr' 'pre'\n",
      " 'preci' 'preh' 'pres' 'press' 'qu' 'que' 'r' 're' 'reat' 'red' 'ree'\n",
      " 'res' 'resp' 'ress' 'ri' 'row' 's' 'se' 'st' 'sy' 't' 'tain' 'te' 'ted'\n",
      " 'ten' 'ter' 'tern' 'ters' 'th' 'the' 'ther' 'ths' 'ti' 'tic' 'tical'\n",
      " 'ties' 'tif' 'til' 'tim' 'ting' 'tion' 'tions' 'tis' 'tit' 'tive' 'tly'\n",
      " 'to' 'tor' 'tr' 'trac' 'truc' 'ts' 'tu' 'tur' 'ty' 'u' 'ual' 'ub' 'uc'\n",
      " 'uch' 'ud' 'ude' 'ue' 'ug' 'ul' 'ular' 'ull' 'uls' 'ult' 'um' 'uman'\n",
      " 'umb' 'un' 'und' 'ung' 'uous' 'up' 'ur' 'ure' 'ured' 'ures' 'us' 'ush'\n",
      " 'ust' 'ut' 'ute' 'uted' 'uth' 'v' 'val' 've' 'ver' 'vers' 'ves' 'w'\n",
      " 'ward' 'wh' 'x' 'y' 'ying' 'yp' 'yr' 'ys' 'yth' 'z' 'ä' 'æ' 'é' 'ë' '▁'\n",
      " '▁1' '▁10' '▁11' '▁12' '▁13' '▁14' '▁15' '▁16' '▁17' '▁18' '▁19' '▁2'\n",
      " '▁20' '▁21' '▁22' '▁23' '▁24' '▁25' '▁26' '▁27' '▁28' '▁3' '▁4' '▁5' '▁6'\n",
      " '▁7' '▁8' '▁9' '▁_' '▁a' '▁ab' '▁abs' '▁ac' '▁acc' '▁ad' '▁add' '▁adm'\n",
      " '▁adv' '▁af' '▁aff' '▁ag' '▁al' '▁all' '▁alle' '▁am' '▁an' '▁anti' '▁ap'\n",
      " '▁app' '▁appr' '▁ar' '▁arr' '▁as' '▁ass' '▁at' '▁att' '▁av' '▁aw' '▁b'\n",
      " '▁bar' '▁be' '▁beh' '▁bel' '▁bl' '▁br' '▁bre' '▁c' '▁cal' '▁cap' '▁car'\n",
      " '▁ch' '▁chr' '▁cir' '▁circ' '▁cl' '▁cle' '▁co' '▁col' '▁com' '▁comm'\n",
      " '▁command' '▁commun' '▁comp' '▁con' '▁conc' '▁conce' '▁cond' '▁conf'\n",
      " '▁cons' '▁cont' '▁contin' '▁contr' '▁cor' '▁coun' '▁counter' '▁cr' '▁cre'\n",
      " '▁d' '▁dar' '▁de' '▁dec' '▁def' '▁del' '▁dem' '▁den' '▁dep' '▁des'\n",
      " '▁desp' '▁deter' '▁dev' '▁develop' '▁dif' '▁dis' '▁disc' '▁disg' '▁diss'\n",
      " '▁dist' '▁div' '▁dr' '▁dre' '▁e' '▁ear' '▁eff' '▁el' '▁ele' '▁em' '▁en'\n",
      " '▁enc' '▁end' '▁ent' '▁ep' '▁equ' '▁er' '▁es' '▁ev' '▁ever' '▁ex' '▁exc'\n",
      " '▁exp' '▁exper' '▁expl' '▁ext' '▁f' '▁fa' '▁fac' '▁fe' '▁fl' '▁for'\n",
      " '▁fore' '▁form' '▁fr' '▁fre' '▁g' '▁gener' '▁ger' '▁gl' '▁god' '▁gr'\n",
      " '▁gre' '▁gu' '▁h' '▁hal' '▁half' '▁hand' '▁happ' '▁he' '▁hear' '▁hel'\n",
      " '▁her' '▁him' '▁hon' '▁human' '▁hyp' '▁ide' '▁ill' '▁im' '▁imm' '▁imp'\n",
      " '▁imper' '▁impr' '▁in' '▁inc' '▁incl' '▁ind' '▁inf' '▁infl' '▁inj' '▁inn'\n",
      " '▁ins' '▁inst' '▁int' '▁inter' '▁inv' '▁ir' '▁is' '▁it' '▁j' '▁je' '▁k'\n",
      " '▁ke' '▁know' '▁l' '▁lab' '▁le' '▁li' '▁life' '▁light' '▁long' '▁m'\n",
      " '▁man' '▁mar' '▁mas' '▁me' '▁mean' '▁met' '▁mis' '▁mo' '▁mod' '▁mor' '▁n'\n",
      " '▁ne' '▁necess' '▁no' '▁non' '▁not' '▁o' '▁ob' '▁obs' '▁occ' '▁of' '▁off'\n",
      " '▁on' '▁op' '▁or' '▁origin' '▁other' '▁out' '▁over' '▁p' '▁par' '▁pass'\n",
      " '▁pat' '▁pe' '▁per' '▁pers' '▁ph' '▁philosoph' '▁phys' '▁pl' '▁pleas'\n",
      " '▁po' '▁pol' '▁pos' '▁poss' '▁power' '▁pr' '▁pre' '▁prec' '▁pred' '▁pres'\n",
      " '▁prof' '▁prop' '▁prot' '▁pur' '▁qu' '▁r' '▁re' '▁read' '▁rec' '▁ref'\n",
      " '▁reg' '▁rel' '▁relig' '▁rem' '▁rep' '▁requ' '▁res' '▁rev' '▁rever' '▁s'\n",
      " '▁sc' '▁sch' '▁se' '▁sec' '▁self' '▁self-' '▁self-d' '▁sens' '▁ser'\n",
      " '▁serv' '▁sh' '▁sign' '▁sim' '▁sk' '▁sl' '▁sm' '▁so' '▁some' '▁sou'\n",
      " '▁soul' '▁sp' '▁spec' '▁spir' '▁spirit' '▁st' '▁str' '▁stup' '▁su' '▁sub'\n",
      " '▁succ' '▁suff' '▁sun' '▁sup' '▁super' '▁sur' '▁sus' '▁sw' '▁sy' '▁sym'\n",
      " '▁t' '▁tal' '▁te' '▁temp' '▁ter' '▁th' '▁the' '▁them' '▁there' '▁ti'\n",
      " '▁tim' '▁tr' '▁trans' '▁tre' '▁truth' '▁tw' '▁tyr' '▁u' '▁un' '▁unc'\n",
      " '▁und' '▁under' '▁une' '▁unf' '▁uns' '▁unw' '▁up' '▁us' '▁v' '▁val' '▁ve'\n",
      " '▁vic' '▁vir' '▁vis' '▁vol' '▁w' '▁we' '▁wh' '▁will' '▁with' '▁wor'\n",
      " '▁world' '▁wr' '▁y']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(target_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "число токенов = 996\n"
     ]
    }
   ],
   "source": [
    "# запишем в tokens все возможные токены\n",
    "tokens = np.unique(target_strings).tolist()\n",
    "tokens.append('<BOS>')\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('число токенов =', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, \"'s\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '--': 8, '--a': 9, '--and': 10, '--as': 11, '--b': 12, '--but': 13, '--d': 14, '--for': 15, '--h': 16, '--he': 17, '--i': 18, '--in': 19, '--is': 20, '--it': 21, '--m': 22, '--n': 23, '--namely': 24, '--or': 25, '--s': 26, '--th': 27, '--that': 28, '--the': 29, '--they': 30, '--this': 31, '--to': 32, '--w': 33, '--we': 34, '--wh': 35, '--what': 36, '--who': 37, '-b': 38, '-c': 39, '-d': 40, '-f': 41, '-g': 42, '-h': 43, '-l': 44, '-like': 45, '-m': 46, '-mor': 47, '-n': 48, '-p': 49, '-r': 50, '-re': 51, '-s': 52, '-t': 53, '-w': 54, '.': 55, '0': 56, '1': 57, '2': 58, '3': 59, '4': 60, '5': 61, '6': 62, '7': 63, '8': 64, '9': 65, ':': 66, ';': 67, '<EOS>': 68, '<PAD>': 69, '=': 70, '?': 71, '[': 72, ']': 73, '_': 74, 'a': 75, 'ab': 76, 'ability': 77, 'able': 78, 'ably': 79, 'ac': 80, 'ace': 81, 'ach': 82, 'ack': 83, 'acr': 84, 'acy': 85, 'ad': 86, 'ade': 87, 'adic': 88, 'ads': 89, 'af': 90, 'ag': 91, 'age': 92, 'ages': 93, 'aid': 94, 'ail': 95, 'ain': 96, 'ained': 97, 'aining': 98, 'ains': 99, 'air': 100, 'ak': 101, 'ake': 102, 'aking': 103, 'al': 104, 'ality': 105, 'aliz': 106, 'all': 107, 'ally': 108, 'als': 109, 'alth': 110, 'am': 111, 'ame': 112, 'amely': 113, 'ament': 114, 'an': 115, 'anc': 116, 'ance': 117, 'ances': 118, 'and': 119, 'ands': 120, 'ane': 121, 'ang': 122, 'ank': 123, 'ann': 124, 'ans': 125, 'ant': 126, 'ants': 127, 'ap': 128, 'aph': 129, 'app': 130, 'aps': 131, 'ar': 132, 'ard': 133, 'are': 134, 'arily': 135, 'arm': 136, 'arn': 137, 'ars': 138, 'art': 139, 'ary': 140, 'as': 141, 'ase': 142, 'ash': 143, 'ason': 144, 'ass': 145, 'ast': 146, 'aste': 147, 'at': 148, 'ate': 149, 'ated': 150, 'ately': 151, 'ates': 152, 'ath': 153, 'ather': 154, 'ati': 155, 'atic': 156, 'ating': 157, 'ation': 158, 'ations': 159, 'atis': 160, 'atisf': 161, 'ative': 162, 'ator': 163, 'ators': 164, 'atter': 165, 'atur': 166, 'atural': 167, 'ature': 168, 'au': 169, 'augh': 170, 'aus': 171, 'av': 172, 'ave': 173, 'avour': 174, 'aw': 175, 'ax': 176, 'ay': 177, 'ays': 178, 'az': 179, 'b': 180, 'bed': 181, 'ber': 182, 'ble': 183, 'br': 184, 'c': 185, 'cc': 186, 'ce': 187, 'ceiv': 188, 'cep': 189, 'ception': 190, 'ch': 191, 'che': 192, 'ci': 193, 'con': 194, 'cr': 195, 'crib': 196, 'cul': 197, 'd': 198, 'der': 199, 'e': 200, 'ear': 201, 'ec': 202, 'ecess': 203, 'ect': 204, 'ection': 205, 'ed': 206, 'edi': 207, 'educ': 208, 'el': 209, 'eless': 210, 'elf': 211, 'ell': 212, 'elop': 213, 'els': 214, 'ely': 215, 'em': 216, 'emb': 217, 'ement': 218, 'emp': 219, 'en': 220, 'enc': 221, 'ence': 222, 'ences': 223, 'ency': 224, 'end': 225, 'ended': 226, 'ending': 227, 'ends': 228, 'ene': 229, 'ened': 230, 'ener': 231, 'eness': 232, 'eng': 233, 'ening': 234, 'ens': 235, 'ent': 236, 'enti': 237, 'ention': 238, 'ently': 239, 'ents': 240, 'ep': 241, 'er': 242, 'ere': 243, 'ered': 244, 'ering': 245, 'ern': 246, 'ers': 247, 'erst': 248, 'ertain': 249, 'erv': 250, 'ery': 251, 'es': 252, 'ess': 253, 'est': 254, 'esti': 255, 'estion': 256, 'et': 257, 'ev': 258, 'ever': 259, 'ex': 260, 'f': 261, 'fer': 262, 'ff': 263, 'fic': 264, 'for': 265, 'form': 266, 'ful': 267, 'fully': 268, 'fulness': 269, 'g': 270, 'ge': 271, 'ged': 272, 'gh': 273, 'gl': 274, 'go': 275, 'gr': 276, 'h': 277, 'hip': 278, 'i': 279, 'ia': 280, 'ial': 281, 'ian': 282, 'ians': 283, 'iation': 284, 'ib': 285, 'ibility': 286, 'ible': 287, 'ic': 288, 'ical': 289, 'ice': 290, 'ich': 291, 'ick': 292, 'ict': 293, 'id': 294, 'idd': 295, 'ide': 296, 'ider': 297, 'ides': 298, 'idity': 299, 'ie': 300, 'ied': 301, 'ief': 302, 'ient': 303, 'ier': 304, 'ies': 305, 'iest': 306, 'iet': 307, 'if': 308, 'ife': 309, 'ific': 310, 'ification': 311, 'ified': 312, 'ig': 313, 'igh': 314, 'ight': 315, 'ights': 316, 'ign': 317, 'il': 318, 'ild': 319, 'ile': 320, 'ility': 321, 'ill': 322, 'ilosoph': 323, 'ilt': 324, 'ily': 325, 'im': 326, 'in': 327, 'ination': 328, 'inc': 329, 'ind': 330, 'ine': 331, 'ined': 332, 'ines': 333, 'iness': 334, 'ing': 335, 'ing-': 336, 'ingly': 337, 'ings': 338, 'ink': 339, 'ins': 340, 'inter': 341, 'ion': 342, 'ions': 343, 'ior': 344, 'ious': 345, 'iously': 346, 'iousness': 347, 'ip': 348, 'ir': 349, 'ire': 350, 'irst': 351, 'is': 352, 'ise': 353, 'ised': 354, 'ises': 355, 'ish': 356, 'ished': 357, 'ishes': 358, 'ishing': 359, 'ising': 360, 'ism': 361, 'ison': 362, 'iss': 363, 'ist': 364, 'isti': 365, 'istic': 366, 'ists': 367, 'it': 368, 'itable': 369, 'itation': 370, 'ite': 371, 'ited': 372, 'ith': 373, 'ither': 374, 'iti': 375, 'itic': 376, 'ities': 377, 'iting': 378, 'ition': 379, 'its': 380, 'itself': 381, 'ity': 382, 'iv': 383, 'ive': 384, 'iver': 385, 'ix': 386, 'iz': 387, 'ize': 388, 'ized': 389, 'izing': 390, 'j': 391, 'jec': 392, 'k': 393, 'ke': 394, 'ked': 395, 'king': 396, 'ks': 397, 'l': 398, 'ld': 399, 'le': 400, 'leas': 401, 'lec': 402, 'led': 403, 'leness': 404, 'les': 405, 'less': 406, 'lessness': 407, 'let': 408, 'li': 409, 'lig': 410, 'light': 411, 'like': 412, 'lim': 413, 'ling': 414, 'lus': 415, 'ly': 416, 'm': 417, 'man': 418, 'me': 419, 'ment': 420, 'ments': 421, 'min': 422, 'ms': 423, 'n': 424, 'ne': 425, 'ner': 426, 'ness': 427, 'ning': 428, 'now': 429, 'o': 430, 'ob': 431, 'oble': 432, 'oc': 433, 'ocr': 434, 'od': 435, 'oduc': 436, 'oes': 437, 'of': 438, 'og': 439, 'ogn': 440, 'oin': 441, 'ok': 442, 'ol': 443, 'old': 444, 'oll': 445, 'olog': 446, 'ology': 447, 'olu': 448, 'olution': 449, 'om': 450, 'oman': 451, 'ome': 452, 'omin': 453, 'omp': 454, 'on': 455, 'ond': 456, 'one': 457, 'ong': 458, 'ons': 459, 'ont': 460, 'oo': 461, 'ood': 462, 'ook': 463, 'ool': 464, 'oom': 465, 'op': 466, 'ope': 467, 'oph': 468, 'or': 469, 'ord': 470, 'ore': 471, 'orm': 472, 'orn': 473, 'ors': 474, 'ort': 475, 'ortun': 476, 'ory': 477, 'os': 478, 'ose': 479, 'oss': 480, 'ost': 481, 'ot': 482, 'oth': 483, 'ou': 484, 'oub': 485, 'ouch': 486, 'ough': 487, 'ought': 488, 'oun': 489, 'ound': 490, 'ounds': 491, 'our': 492, 'ours': 493, 'ous': 494, 'ously': 495, 'out': 496, 'ov': 497, 'ove': 498, 'oved': 499, 'over': 500, 'ow': 501, 'owed': 502, 'ower': 503, 'owing': 504, 'own': 505, 'ows': 506, 'ox': 507, 'oy': 508, 'p': 509, 'par': 510, 'pec': 511, 'pect': 512, 'per': 513, 'ph': 514, 'pir': 515, 'pl': 516, 'plic': 517, 'pos': 518, 'position': 519, 'pr': 520, 'pre': 521, 'preci': 522, 'preh': 523, 'pres': 524, 'press': 525, 'qu': 526, 'que': 527, 'r': 528, 're': 529, 'reat': 530, 'red': 531, 'ree': 532, 'res': 533, 'resp': 534, 'ress': 535, 'ri': 536, 'row': 537, 's': 538, 'se': 539, 'st': 540, 'sy': 541, 't': 542, 'tain': 543, 'te': 544, 'ted': 545, 'ten': 546, 'ter': 547, 'tern': 548, 'ters': 549, 'th': 550, 'the': 551, 'ther': 552, 'ths': 553, 'ti': 554, 'tic': 555, 'tical': 556, 'ties': 557, 'tif': 558, 'til': 559, 'tim': 560, 'ting': 561, 'tion': 562, 'tions': 563, 'tis': 564, 'tit': 565, 'tive': 566, 'tly': 567, 'to': 568, 'tor': 569, 'tr': 570, 'trac': 571, 'truc': 572, 'ts': 573, 'tu': 574, 'tur': 575, 'ty': 576, 'u': 577, 'ual': 578, 'ub': 579, 'uc': 580, 'uch': 581, 'ud': 582, 'ude': 583, 'ue': 584, 'ug': 585, 'ul': 586, 'ular': 587, 'ull': 588, 'uls': 589, 'ult': 590, 'um': 591, 'uman': 592, 'umb': 593, 'un': 594, 'und': 595, 'ung': 596, 'uous': 597, 'up': 598, 'ur': 599, 'ure': 600, 'ured': 601, 'ures': 602, 'us': 603, 'ush': 604, 'ust': 605, 'ut': 606, 'ute': 607, 'uted': 608, 'uth': 609, 'v': 610, 'val': 611, 've': 612, 'ver': 613, 'vers': 614, 'ves': 615, 'w': 616, 'ward': 617, 'wh': 618, 'x': 619, 'y': 620, 'ying': 621, 'yp': 622, 'yr': 623, 'ys': 624, 'yth': 625, 'z': 626, 'ä': 627, 'æ': 628, 'é': 629, 'ë': 630, '▁': 631, '▁1': 632, '▁10': 633, '▁11': 634, '▁12': 635, '▁13': 636, '▁14': 637, '▁15': 638, '▁16': 639, '▁17': 640, '▁18': 641, '▁19': 642, '▁2': 643, '▁20': 644, '▁21': 645, '▁22': 646, '▁23': 647, '▁24': 648, '▁25': 649, '▁26': 650, '▁27': 651, '▁28': 652, '▁3': 653, '▁4': 654, '▁5': 655, '▁6': 656, '▁7': 657, '▁8': 658, '▁9': 659, '▁_': 660, '▁a': 661, '▁ab': 662, '▁abs': 663, '▁ac': 664, '▁acc': 665, '▁ad': 666, '▁add': 667, '▁adm': 668, '▁adv': 669, '▁af': 670, '▁aff': 671, '▁ag': 672, '▁al': 673, '▁all': 674, '▁alle': 675, '▁am': 676, '▁an': 677, '▁anti': 678, '▁ap': 679, '▁app': 680, '▁appr': 681, '▁ar': 682, '▁arr': 683, '▁as': 684, '▁ass': 685, '▁at': 686, '▁att': 687, '▁av': 688, '▁aw': 689, '▁b': 690, '▁bar': 691, '▁be': 692, '▁beh': 693, '▁bel': 694, '▁bl': 695, '▁br': 696, '▁bre': 697, '▁c': 698, '▁cal': 699, '▁cap': 700, '▁car': 701, '▁ch': 702, '▁chr': 703, '▁cir': 704, '▁circ': 705, '▁cl': 706, '▁cle': 707, '▁co': 708, '▁col': 709, '▁com': 710, '▁comm': 711, '▁command': 712, '▁commun': 713, '▁comp': 714, '▁con': 715, '▁conc': 716, '▁conce': 717, '▁cond': 718, '▁conf': 719, '▁cons': 720, '▁cont': 721, '▁contin': 722, '▁contr': 723, '▁cor': 724, '▁coun': 725, '▁counter': 726, '▁cr': 727, '▁cre': 728, '▁d': 729, '▁dar': 730, '▁de': 731, '▁dec': 732, '▁def': 733, '▁del': 734, '▁dem': 735, '▁den': 736, '▁dep': 737, '▁des': 738, '▁desp': 739, '▁deter': 740, '▁dev': 741, '▁develop': 742, '▁dif': 743, '▁dis': 744, '▁disc': 745, '▁disg': 746, '▁diss': 747, '▁dist': 748, '▁div': 749, '▁dr': 750, '▁dre': 751, '▁e': 752, '▁ear': 753, '▁eff': 754, '▁el': 755, '▁ele': 756, '▁em': 757, '▁en': 758, '▁enc': 759, '▁end': 760, '▁ent': 761, '▁ep': 762, '▁equ': 763, '▁er': 764, '▁es': 765, '▁ev': 766, '▁ever': 767, '▁ex': 768, '▁exc': 769, '▁exp': 770, '▁exper': 771, '▁expl': 772, '▁ext': 773, '▁f': 774, '▁fa': 775, '▁fac': 776, '▁fe': 777, '▁fl': 778, '▁for': 779, '▁fore': 780, '▁form': 781, '▁fr': 782, '▁fre': 783, '▁g': 784, '▁gener': 785, '▁ger': 786, '▁gl': 787, '▁god': 788, '▁gr': 789, '▁gre': 790, '▁gu': 791, '▁h': 792, '▁hal': 793, '▁half': 794, '▁hand': 795, '▁happ': 796, '▁he': 797, '▁hear': 798, '▁hel': 799, '▁her': 800, '▁him': 801, '▁hon': 802, '▁human': 803, '▁hyp': 804, '▁ide': 805, '▁ill': 806, '▁im': 807, '▁imm': 808, '▁imp': 809, '▁imper': 810, '▁impr': 811, '▁in': 812, '▁inc': 813, '▁incl': 814, '▁ind': 815, '▁inf': 816, '▁infl': 817, '▁inj': 818, '▁inn': 819, '▁ins': 820, '▁inst': 821, '▁int': 822, '▁inter': 823, '▁inv': 824, '▁ir': 825, '▁is': 826, '▁it': 827, '▁j': 828, '▁je': 829, '▁k': 830, '▁ke': 831, '▁know': 832, '▁l': 833, '▁lab': 834, '▁le': 835, '▁li': 836, '▁life': 837, '▁light': 838, '▁long': 839, '▁m': 840, '▁man': 841, '▁mar': 842, '▁mas': 843, '▁me': 844, '▁mean': 845, '▁met': 846, '▁mis': 847, '▁mo': 848, '▁mod': 849, '▁mor': 850, '▁n': 851, '▁ne': 852, '▁necess': 853, '▁no': 854, '▁non': 855, '▁not': 856, '▁o': 857, '▁ob': 858, '▁obs': 859, '▁occ': 860, '▁of': 861, '▁off': 862, '▁on': 863, '▁op': 864, '▁or': 865, '▁origin': 866, '▁other': 867, '▁out': 868, '▁over': 869, '▁p': 870, '▁par': 871, '▁pass': 872, '▁pat': 873, '▁pe': 874, '▁per': 875, '▁pers': 876, '▁ph': 877, '▁philosoph': 878, '▁phys': 879, '▁pl': 880, '▁pleas': 881, '▁po': 882, '▁pol': 883, '▁pos': 884, '▁poss': 885, '▁power': 886, '▁pr': 887, '▁pre': 888, '▁prec': 889, '▁pred': 890, '▁pres': 891, '▁prof': 892, '▁prop': 893, '▁prot': 894, '▁pur': 895, '▁qu': 896, '▁r': 897, '▁re': 898, '▁read': 899, '▁rec': 900, '▁ref': 901, '▁reg': 902, '▁rel': 903, '▁relig': 904, '▁rem': 905, '▁rep': 906, '▁requ': 907, '▁res': 908, '▁rev': 909, '▁rever': 910, '▁s': 911, '▁sc': 912, '▁sch': 913, '▁se': 914, '▁sec': 915, '▁self': 916, '▁self-': 917, '▁self-d': 918, '▁sens': 919, '▁ser': 920, '▁serv': 921, '▁sh': 922, '▁sign': 923, '▁sim': 924, '▁sk': 925, '▁sl': 926, '▁sm': 927, '▁so': 928, '▁some': 929, '▁sou': 930, '▁soul': 931, '▁sp': 932, '▁spec': 933, '▁spir': 934, '▁spirit': 935, '▁st': 936, '▁str': 937, '▁stup': 938, '▁su': 939, '▁sub': 940, '▁succ': 941, '▁suff': 942, '▁sun': 943, '▁sup': 944, '▁super': 945, '▁sur': 946, '▁sus': 947, '▁sw': 948, '▁sy': 949, '▁sym': 950, '▁t': 951, '▁tal': 952, '▁te': 953, '▁temp': 954, '▁ter': 955, '▁th': 956, '▁the': 957, '▁them': 958, '▁there': 959, '▁ti': 960, '▁tim': 961, '▁tr': 962, '▁trans': 963, '▁tre': 964, '▁truth': 965, '▁tw': 966, '▁tyr': 967, '▁u': 968, '▁un': 969, '▁unc': 970, '▁und': 971, '▁under': 972, '▁une': 973, '▁unf': 974, '▁uns': 975, '▁unw': 976, '▁up': 977, '▁us': 978, '▁v': 979, '▁val': 980, '▁ve': 981, '▁vic': 982, '▁vir': 983, '▁vis': 984, '▁vol': 985, '▁w': 986, '▁we': 987, '▁wh': 988, '▁will': 989, '▁with': 990, '▁wor': 991, '▁world': 992, '▁wr': 993, '▁y': 994, '<BOS>': 995}\n"
     ]
    }
   ],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "print(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(target_strings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(strings):\n",
    "    matrix = np.array(np.empty(shape=(0, len(strings[0])), dtype=np.int_))\n",
    "    for str in strings:\n",
    "        line_str = []\n",
    "        for c in str:\n",
    "            line_str.append(token_to_id[c])\n",
    "        matrix = np.append(matrix, [np.array(line_str, dtype=np.int_)], axis=0)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[888 261  81  68  69  69  69  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69  69  69  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69]\n",
      " [944 518 335 956 148 965 826 661 986 451  36 956 220  71 826 959 856 789\n",
      "  490  68  69  69  69  69  69  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69]\n",
      " [779 947 511 561 956 148 674 878 247   6 812 928 774 132 684 957 620 792\n",
      "  173 692 220  68  69  69  69  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69]\n",
      " [729 439 417 160 573   6 792 173 774  95 206 951 430 971 248 119 986 450\n",
      "  220  28 957 955 528 287  68  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69]\n",
      " [920 347 631 119 706 591 541 809 476 382 990 988 291 957 620 792 173 978\n",
      "  577 108 870  94  68  69  69  69  69  69  69  69  69  69  69  69  69  69\n",
      "   69  69  69  69  69]]\n"
     ]
    }
   ],
   "source": [
    "print(to_matrix(target_strings[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matrix = to_matrix(target_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr29VFWBELHv"
   },
   "source": [
    "Получим то, что подается на вход нейросети. Из таргет-строк получите строки тех же длин, которые начинаются с символа \\<BOS\\> и будут без последнего символа таргет-строки. Полученные строки также должны состоять из чисел, как и ранее посчитанные таргет-строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁pre', 'f', 'ace', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(target_strings[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XTDInid-DYoN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<BOS>', '▁pre', 'f', 'ace', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['<BOS>', '▁sup', 'pos', 'ing', '▁th', 'at', '▁truth', '▁is', '▁a', '▁w', 'oman', '--what', '▁th', 'en', '?', '▁is', '▁there', '▁not', '▁gr', 'ound', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['<BOS>', '▁for', '▁sus', 'pec', 'ting', '▁th', 'at', '▁all', '▁philosoph', 'ers', ',', '▁in', '▁so', '▁f', 'ar', '▁as', '▁the', 'y', '▁h', 'ave', '▁be', 'en', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['<BOS>', '▁d', 'og', 'm', 'atis', 'ts', ',', '▁h', 'ave', '▁f', 'ail', 'ed', '▁t', 'o', '▁und', 'erst', 'and', '▁w', 'om', 'en', '--that', '▁the', '▁ter', 'r', 'ible', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['<BOS>', '▁ser', 'iousness', '▁', 'and', '▁cl', 'um', 'sy', '▁imp', 'ortun', 'ity', '▁with', '▁wh', 'ich', '▁the', 'y', '▁h', 'ave', '▁us', 'u', 'ally', '▁p', 'aid', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n"
     ]
    }
   ],
   "source": [
    "feature_strings = []\n",
    "for str in target_strings:\n",
    "    feature_str = ['<BOS>'] + str[:-1]\n",
    "    feature_strings.append(feature_str)\n",
    "    \n",
    "print(feature_strings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[995, 888, 261,  81,  68,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69],\n",
       "       [995, 944, 518, 335, 956, 148, 965, 826, 661, 986, 451,  36, 956,\n",
       "        220,  71, 826, 959, 856, 789, 490,  68,  69,  69,  69,  69,  69,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69],\n",
       "       [995, 779, 947, 511, 561, 956, 148, 674, 878, 247,   6, 812, 928,\n",
       "        774, 132, 684, 957, 620, 792, 173, 692, 220,  68,  69,  69,  69,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69],\n",
       "       [995, 729, 439, 417, 160, 573,   6, 792, 173, 774,  95, 206, 951,\n",
       "        430, 971, 248, 119, 986, 450, 220,  28, 957, 955, 528, 287,  68,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69],\n",
       "       [995, 920, 347, 631, 119, 706, 591, 541, 809, 476, 382, 990, 988,\n",
       "        291, 957, 620, 792, 173, 978, 577, 108, 870,  94,  68,  69,  69,\n",
       "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
       "         69,  69]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = to_matrix(feature_strings)\n",
    "feature_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th1Dp1Q05ZWk"
   },
   "source": [
    "Разбейте полученные данные на обучение и валидацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DqFLzfKm5z0b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(feature_matrix, target_matrix, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFc9wY_Z5ip2"
   },
   "source": [
    "Приступим к написанию модели. Ваша модель должна состоять из эмбеддинг слоя, рекуррентного слоя (по желанию, возможно несколько слоев) и линейного слоя, который будет применяться к каждому скрытому состоянию рекуррентного слоя.\n",
    "\n",
    "В качестве рекуррентного слоя (слоев) лучше использовать `nn.GRU`, `nn.GRUCell`, `nn.LSTM` или `nn.LSTMCell` по вашему желанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем 2 модели: простую и модель, которая сначала после эмбеддинга делает одномерные свертки, чтоб модель могла понять, какие признаки лучше, а только потом рекуррентную нейронную сеть. Забегая вперед, скажу, что вторая модель показала потрясающую точность (в районе 60%), но, кажется, это из-за того, что мы фильтром заглядывали вперед. Далее эта модель просто зацикливала слова, то есть, ничего хорошего с нее не получилось. Поэтому будем использовать простую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    ''' Класс модели для генерации имён на основе LSTM, \n",
    "    которая принимает на вход предыдущее скрытое состояние '''\n",
    "    \n",
    "    def __init__(self, num_tokens=num_tokens, emb_size=64, rnn_num_units=256):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=rnn_num_units, batch_first=True)\n",
    "        self.bn = nn.BatchNorm1d(rnn_num_units)\n",
    "        self.hid_to_logit = nn.Linear(rnn_num_units, num_tokens)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None, device='cuda'):\n",
    "        # x.size = [batch_size, max_name_len]\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "\n",
    "        x = x.to(device)\n",
    "        # emb.size = [batch_size, max_name_len, emb_size]\n",
    "        emb = self.emb(x)\n",
    "        # h_seq.size = [batch_size, max_name_len, rnn_num_units]\n",
    "        if h0 is not None:\n",
    "            output, h = self.rnn(emb, (h0, c0))\n",
    "        else:\n",
    "            output, h = self.rnn(emb)\n",
    "\n",
    "        # next_logits.size = [batch_size, max_name_len, num_tokens]\n",
    "        next_logits = self.hid_to_logit(self.bn(output.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "\n",
    "        # next_logits.size = [batch_size, max_name_len, num_tokens]\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G4qk8NGPCNIZ"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    ''' Класс модели для генерации имён на основе LSTM, \n",
    "    которая принимает на вход предыдущее скрытое состояние '''\n",
    "    \n",
    "    def __init__(self, num_tokens=num_tokens, emb_size=256, rnn_num_units=1024, n_filters=256, filter_sizes=[3, 3]):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "          nn.Conv1d(\n",
    "              in_channels=emb_size, out_channels=n_filters, \n",
    "              kernel_size=fs, padding=1,\n",
    "          ) for fs in filter_sizes\n",
    "        ])\n",
    "        self.rnn = nn.LSTM(input_size=len(filter_sizes) * n_filters, hidden_size=rnn_num_units, batch_first=True)\n",
    "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h0=None, c0=None, device='cuda'):\n",
    "        # x.size = [batch_size, max_name_len]\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "\n",
    "        x = x.to(device)\n",
    "        # emb.size = [batch_size, max_name_len, emb_size]\n",
    "        emb = self.emb(x)\n",
    "        # emb.size = [batch_size, emb_size, max_name_len]\n",
    "        emb = emb.permute(0, 2, 1)\n",
    "        # con.size = [batch_size, n_filters, max_name_len - filter_sizes[n] + 3]\n",
    "        con = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        cat = torch.cat(con, dim = 1)\n",
    "#         con = torch.tensor([F.relu(conv(emb)).cpu().detach().numpy() for conv in self.convs]).cuda()\n",
    "#         print(cat)\n",
    "        # con.size = [batch_size, max_name_len - filter_sizes[n] + 3, n_filters]\n",
    "        cat = cat.permute(0, 2, 1)\n",
    "        # h_seq.size = [batch_size, max_name_len, rnn_num_units]\n",
    "        if h0 is not None:\n",
    "            output, (h, c) = self.rnn(cat, (h0, c0))\n",
    "        else:\n",
    "            output, (h, c) = self.rnn(cat)\n",
    "\n",
    "        # next_logits.size = [batch_size, max_name_len, num_tokens]\n",
    "        next_logits = self.hid_to_logits(output)\n",
    "\n",
    "        # next_logits.size = [batch_size, max_name_len, num_tokens]\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fruPdins7TPb"
   },
   "source": [
    "Напишите функцию вычисления лосса. \n",
    "\n",
    "**Замечание.**\n",
    "\n",
    "Добавленные при помощи паддинга символы не должны оказывать влияния на лосс. То есть по позициям, где истинный символ является специальным символом для паддинга, при вычислении лосса не нужно обращать внимания на предсказания. В противном случае нейронная сеть может научиться всегда предсказывать этот специальный символ из-за того, что он часто встречается. \n",
    "\n",
    "Однако, лосс по символу '\\<EOS\\>' должен быть посчитан и добавлен к финальному лоссу.\n",
    "\n",
    "Например, можно посчитать лосс следующим образом:\n",
    "\n",
    "`criterion = nn.CrossEntropyLoss(ignore_index=pad_ix, size_average=True)`\n",
    "\n",
    "`criterion(logits, y_batch)`\n",
    "\n",
    "вместо\n",
    "\n",
    "`F.cross_entropy(logits, y_batch)`\n",
    "\n",
    "Здесь `pad_ix` — число, которое мы поставили в соответствие специальному символу для паддинга ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "00nujYac86hK"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, X_batch, y_batch):\n",
    "    pad_idx = token_to_id['<PAD>']\n",
    "    logits, _ = model(X_batch)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, size_average=True)\n",
    "    loss = criterion(logits.view(-1, num_tokens), y_batch.view(-1).to(device))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFviN_QJ7cjZ"
   },
   "source": [
    "Напишем функцию генерации батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2KbDaPXK7c-Z"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batch_size, shuffle=True):\n",
    "    \"\"\" Генератор случайных батчей \"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(np.arange(len(X)))\n",
    "    else:\n",
    "        indices = np.arange(len(X))\n",
    "        \n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        ix = indices[start: start + batch_size]\n",
    "        yield X[ix], y[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diOfP7gn7NTq"
   },
   "source": [
    "Инициализируем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jGborO_QLdDx"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yak0RMMz7lnN"
   },
   "source": [
    "Обучение. Это может занять некоторое время. Постройте также график кривой обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Возвращает точность модели.\n",
    "\n",
    "    Параметры.\n",
    "    1) preds — предсказания модели,\n",
    "    2) y — истинные метки классов.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    binary_preds = np.argmax(preds, axis=2)\n",
    "    \n",
    "    # округляет предсказания до ближайшего integer\n",
    "    acc = accuracy_score(y.reshape(-1), binary_preds.reshape(-1))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "AHML-rJ1Kj44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 30 took 1.641s\n",
      "  training loss (in-iteration): \t3.332850\n",
      "  validation accuracy: \t\t\t16.33 %\n",
      "CPU times: total: 50.2 s\n",
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.3)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Обучение\n",
    "    model.train(True)\n",
    "    train_loss = []\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size, True):\n",
    "        X_batch = torch.LongTensor(X_batch)\n",
    "        y_batch = torch.LongTensor(y_batch)\n",
    "        \n",
    "        # Считаем функцию потерь\n",
    "        loss = compute_loss(model, X_batch, y_batch)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # делаем backprop\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    # Валидация\n",
    "    model.train(False)\n",
    "    val_acc = []\n",
    "    with torch.no_grad(): # отключим подсчёт градиентов на валидации\n",
    "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size, False):\n",
    "            X_batch = torch.tensor(X_batch, dtype=torch.int64)\n",
    "            logits, _ = model(X_batch)\n",
    "            val_loss = compute_loss(model, X_batch, torch.LongTensor(y_batch))\n",
    "            scheduler.step(val_loss)\n",
    "            acc = binary_accuracy(logits.to('cpu').numpy(), y_batch)\n",
    "            val_acc.append(acc)\n",
    "            \n",
    "            \n",
    "    clear_output(True)\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    \n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_acc[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не очень радуют. С другой стороны, среди почти 1000 токенов модель угадывает правильно 1 раз из 6, что в принципе сильно лучше, чем если бы мы случайно выбирали некоторый токен. Посмотрим, что будет в другой части задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dt4nSCj7sOz"
   },
   "source": [
    "Напишите функцию, которая по началу высказывания будет генерировать следующие `length_to_predict` символов:\n",
    "* Входной текст приведите к формату, который принимает на вход модель. \n",
    "* Предскажите первый символ. \n",
    "* Предсказаный символ следует добавить в конец текста и полученный текст опять привести к нужному формату. Альтернативно, вы можете прогнать лишь одну ячейку рекуррентной сети, на вход которой подать только что предсказанный символ и в качестве предыдущего скрытого состояния которой взять скрытое состояние полученное на предыдущем шаге.\n",
    "* Затем можно предсказать следующий символ и так далее. \n",
    "\n",
    "\n",
    "*Замечания.*\n",
    "\n",
    "1. Модель принимает объекты батчами, поэтому если вы будете подавать ей один элемент она может ругаться. Один элемент нужно подавать как батч размера 1.\n",
    "\n",
    "2. Для получения предсказания в виде индекса следующего символа нужно найти аргмаксимум по логитам. В этом вам поможет `logits.max(dim)`, который принимает в аргументах размерность `dim` по которой искать максимумы и возвращает сразу 2 тензора — максимумы и индексы максимумов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf8J0J2deRMF"
   },
   "source": [
    "Сначала реализуйте жадную генерации текста — на каждом шаге берите токен с наибольшей вероятностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "id": "TXsJey9JsCmz"
   },
   "outputs": [],
   "source": [
    "def simple_predict(model, text, length_to_predict):\n",
    "    bpe_string = bpe.encode(text, output_type=yttm.OutputType.SUBWORD, bos=False, eos=False)\n",
    "    generated_bpe_string = bpe_string.copy()\n",
    "    generated_matrix = torch.LongTensor(to_matrix([generated_bpe_string]))\n",
    "    token_id = 0\n",
    "    logits = []\n",
    "    while len(generated_matrix[0]) < length_to_predict and generated_matrix[0][-1] != token_to_id['<EOS>']:\n",
    "        if token_id == 0:\n",
    "            logp_next, (h, c) = model(generated_matrix)\n",
    "        else:\n",
    "            logp_next, (h, c) = model(generated_matrix[-1:], h, c)\n",
    "        p_next = F.softmax(logp_next[:, -1, :], dim=-1).data.cpu().numpy()[0, :]\n",
    "        \n",
    "        next_idx = np.argmax(p_next)\n",
    "        logits.append(np.log(np.max(p_next)))\n",
    "        next_idx = torch.LongTensor([[next_idx]])\n",
    "        generated_matrix = torch.cat([generated_matrix, next_idx], dim=1)\n",
    "        \n",
    "    generated_bpe_string = ''.join([tokens[idx] for idx in generated_matrix.data.numpy()[0]])\n",
    "    return [bpe.subword_to_id(c) for c in generated_bpe_string], logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5oqJDcZ8RBr"
   },
   "source": [
    "Посмотрим, что предсказывает модель. Не пугайтесь, если будет не много смысла в самих высказываниях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.state_dict(), 'epoch': epoch}, 'rnn_conv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (emb): Embedding(996, 256)\n",
       "  (rnn): LSTM(256, 1024, batch_first=True)\n",
       "  (hid_to_logits): Linear(in_features=1024, out_features=996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "id": "x03u2JPqsGyb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 24, 5, 4, 8, 12, 4, 18, 6, 7, 9, 10, 27, 5, 4, 10, 24, 12, 16, 9, 4, 6, 9, 44, 4, 58, 14, 5, 25, 5, 13, 12, 17, 16, 18, 6, 8, 19, 56, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['be no mistake about it: \"developuming,<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, log_probs = simple_predict(model, \"be no mistake about it:\", 50)\n",
    "print(text)\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "id": "nIosji1-8V_B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial intelegence is interesting to the point of view of the<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, log_probs = simple_predict(model, \"artificial intelegence is interesting\", 50)\n",
    "\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "Ir4MCMXR8YY8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people love ml and has mere<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, log_probs = simple_predict(model, 'people love ml and', 45)\n",
    "\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель часто вставляет неизвестный токен. Также стоит отметить, что даже когда модель его не вставляет, получается белиберда. Будем надеяться, что после Beam-search'а будет лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFtp-0MTcysK"
   },
   "source": [
    "Жадная генерация не самая хорошая. Возможно ваши предсказания зациклились. Это связано с тем, что \n",
    "1. Модель в основном смотрит лишь на последние несколько токенов при предсказании следующего.\n",
    "\n",
    "2. При предсказании модель генерирует жадно и максимизирует вероятность только одного следующего токена, не думая про то, что может быть дальше.\n",
    "\n",
    "3. Модель была обучена максимизировать правдоподобие и генерировать токены смотря лишь на предыдущие токены. Из этого модель сильно теряет в разнообразии и может получится так, что модель найдет некоторые часто повторяющиеся структуры в датасете и будет присваивать им большую вероятность. \n",
    "\n",
    "Для решения этой проблемы существует несколько методов. Одни из них:\n",
    "\n",
    "* Top k sampling \n",
    "\n",
    "  Для предсказания следующего токена посмотрим на  распределение вероятностей на следующий токен. Выбираем $k$ токенов с максимальной вероятностью и из них сэмплируем один токен с вероятностью пропорциональной предсказанной для этого токена вероятности.\n",
    "\n",
    "* Beam-search\n",
    "\n",
    "\n",
    "\\\\\n",
    "\n",
    "**Реализуйте один из подходов.** \n",
    "\n",
    "* Top k sampling (1 балл)\n",
    "\n",
    "* Beam-search (3 балла)\n",
    "\n",
    "Учитывайте, что реализовать beam-search намного сложнее, чем top k sampling, поэтому за его реализацию дается больше баллов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = []\n",
    "v.append([1, 2])\n",
    "v.append([3, 4])\n",
    "np.array(v).reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([1, 2, 3])\n",
    "idx = [b != 2]\n",
    "b[idx][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "id": "K4gryi7egkrt"
   },
   "outputs": [],
   "source": [
    "def predict(model, text, length_to_predict, k=100, m=10000):\n",
    "    bpe_string = bpe.encode(text, output_type=yttm.OutputType.SUBWORD, bos=False, eos=False)\n",
    "    generated_bpe_string = bpe_string.copy()\n",
    "    generated_matrix = torch.LongTensor(to_matrix([generated_bpe_string]))\n",
    "    token_id = -1\n",
    "    beam_search_variants = generated_matrix\n",
    "    beam_search_probs = []\n",
    "    best_comb = generated_matrix[0]\n",
    "    while len(best_comb) < length_to_predict and best_comb[-1] != token_to_id['<EOS>']:\n",
    "        token_id += 1\n",
    "        if token_id == 0:\n",
    "            logp_next, (h, c) = model(generated_matrix)\n",
    "            p_next = F.softmax(logp_next[:, -1, :], dim=-1).data.cpu().numpy()[0, :]\n",
    "            top_k_variants = np.flip(np.argsort(p_next))[:k]\n",
    "            top_k_probs = np.flip(np.sort(p_next))[:k]\n",
    "            beam_search_variants = beam_search_variants.numpy().reshape(-1)\n",
    "            beam_search_variants = [np.append(beam_search_variants, var) for var in top_k_variants]\n",
    "            beam_search_probs = [prob for prob in top_k_probs]\n",
    "            continue\n",
    "        old_beam_search_variants = beam_search_variants\n",
    "        old_beam_search_probs = beam_search_probs\n",
    "        beam_search_variants = []\n",
    "        beam_search_probs = []\n",
    "#         print(old_beam_search_variants)\n",
    "        for beam_var, beam_prob in zip(old_beam_search_variants, old_beam_search_probs):\n",
    "            logp_next, (h, c) = model(torch.LongTensor(beam_var[-1:]).reshape(1, -1), h, c)\n",
    "            p_next = F.softmax(logp_next[:, -1, :], dim=-1).data.cpu().numpy()[0, :]\n",
    "            top_k_variants = np.flip(np.argsort(p_next))[:k]\n",
    "            tok_k_probs = np.flip(np.sort(p_next))[:k]\n",
    "            beam_search_variants.append([np.append(beam_var, var) for var in top_k_variants])\n",
    "            beam_search_probs.append([beam_prob * prob for prob in top_k_probs])\n",
    "        beam_search_variants = beam_search_variants[0]\n",
    "#         print(beam_search_variants)\n",
    "#         print(beam_search_probs)\n",
    "        beam_search_probs = np.array(beam_search_probs).reshape(-1).tolist()\n",
    "#         print(beam_search_probs)\n",
    "#         print(token_id)\n",
    "#         print(beam_search_variants)\n",
    "\n",
    "        if len(beam_search_variants) > m:\n",
    "            top_m_idxs = np.flip(np.argsort(beam_search_probs))[:m]\n",
    "            beam_search_variants = beam_search_variants[top_m_idxs]\n",
    "            beam_search_probs = np.flip(np.sort(beam_search_probs))[:m]\n",
    "            \n",
    "        best_comb = beam_search_variants[0]\n",
    "        \n",
    "    \n",
    "    generated_bpe_string = ''.join([tokens[idx] for idx in best_comb])\n",
    "    return [bpe.subword_to_id(c) for c in generated_bpe_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁'"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.id_to_subword(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be no mistake about it: \" profound in the thatsimshism age-mid cur forward, the law: they<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = predict(model, \"be no mistake about it:\", 50)\n",
    "\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial intelegence is interesting to in when obediocrever and year the wildisonscil-and thems,<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = predict(model, \"artificial intelegence is interesting\", 50)\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people love ml and hasats of the evoach many to accord, as god,<UNK><UNK><UNK><UNK><UNK>']"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = predict(model, 'people love ml and', 50)\n",
    "\n",
    "bpe.decode([list(np.array(text))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После beam search'а модель стала иногда предсказывать что-то осмысленное. Хотя проблема с токенами `<UNK>` осталась. В любом случае, beam search работает лучше, чем наивная модель предсказывания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMOQ_maBr9t5"
   },
   "source": [
    "### **Дополнительная информация**\n",
    "\n",
    "[Статья](https://www.aclweb.org/anthology/P19-1365.pdf) с ACL 2019 про сравнение различных методов декодирования."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of task-ml2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
