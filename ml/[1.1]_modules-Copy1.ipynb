{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIDgsmtTLT89"
   },
   "source": [
    "См. сначала **`[0.1]_train_model.ipynb`**, в котором описана структура этого задания.  \n",
    "\n",
    "После вполнения этого ноутбука см. **`[2.1]_test_modules.ipynb`** для проверки работы всех компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc6-Qyju5HaT"
   },
   "source": [
    "## Важно\n",
    "Этот ноутбук является продолжением ноутбука `[1]_task8_modules.ipynb`.  \n",
    "\n",
    "Все модули из прошлого задания уже реализованы. Теперь нужно реализовать слои `Dropout` для борьбы с переобучением, `Batchnorm` и `Scaling` для реализации слоя батч-нормализации, `Flatten` для преобразования размерностей тензоров, две функции активации `LeakyReLU`и `ELU`, и, наконец, любой из оптимизаторов, которые мы разобрали на прошлом занятии, кроме обычного `SGD`. \n",
    "\n",
    "Обратите внимание на то, что был дополнен модуль `Module`. Был добавлен атрибут `trainig`, а также методы `train` и `evaluate`. Это нововведение связано с тем, что `Dropout` и `Batchnorm` работают по-разному в фазах обучения и применения обученной модели.\n",
    "\n",
    "Если вы выполняли прошлое задание, то можно сразу  *перейти к блоку с реализацией*, пропустив блок с мотивацией и проектированием фреймворка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQjtx_23LT9E"
   },
   "source": [
    "## Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8NbmAcwLT9F"
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/2f/49/84/2f4984329848be3825c17672beef797e.png\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KAXzni-LT9G"
   },
   "source": [
    "Мы хотим построить \"с нуля\" свой мини-фреймворк для обучения нейронных сетей. Он должен позволять создавать, обучать и тестировать нейросети. Как известно из лекции и семинара, **цикл обучения нейросети** выглядит так:\n",
    "\n",
    "```\n",
    "# однослойная нейросеть \n",
    "model = Sequential()\n",
    "model.add(Linear(2,2))\n",
    "model.add(LogSoftMax())\n",
    "\n",
    "criterion = NLLCriterion()\n",
    "\n",
    "optimizer = SGD(lr=1e-2, momentum=0.9)\n",
    "\n",
    "# одна эпоха -- один проход по обучающей выборке\n",
    "for i in range(n_epoch):\n",
    "    # одна итерация -- один батч\n",
    "    for x_batch, y_batch in train_generator(sample, labels, batch_size):\n",
    "        # Обнуляем градиенты с предыдущей итерации\n",
    "        model.zero_grad_params()\n",
    "        # Forward pass\n",
    "        predictions = model.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "        # Backward pass\n",
    "        last_grad_input = criterion.backward(predictions, y_batch)\n",
    "        model.backward(x_batch, last_grad_input)\n",
    "        # Обновление весов\n",
    "        optimizer(\n",
    "            model.get_params(), \n",
    "            model.get_grad_params(), \n",
    "            opt_params,\n",
    "            opt_state\n",
    "        )\n",
    " ```\n",
    " \n",
    "Одна итерация внутреннего цикла называется **одной итерацией обучения нейросети**. Одна итерация внешнего цикла называется **одной эпохой обучения нейросети**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMqw-W-6LT9H"
   },
   "source": [
    "## Проектирование фреймворка "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS4c3RDgLT9H"
   },
   "source": [
    "### Базовые концепции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVT7CxlfLT9I"
   },
   "source": [
    "**Нейросеть** $-$ это последовательность слоев. В реализации ее удобно представлять абстракцией `Sequential`. \n",
    "\n",
    "**Слой** $-$ это некоторая функция, у которой в общем случае есть обучаемые параметры. Есть слои и без обучаемых параметров (например, функции активации, SoftMax, LogSoftMax), однако все эти функции все равно удобно называть слоями нейросети. В реализации один слой удобно представлять абстракцией `Module`.  Например, `Sequential(Linear, ReLU)` -- это три уже модуля.\n",
    "\n",
    "Каждый слой должен уметь делать прямой проход **forward pass**, и обратный проход **backward pass**. В реализации forward pass удобно представлять абстрактным методом `forward()`, backward pass удобно представлять абстрактным методом `backward()`.\n",
    "\n",
    "<img src=\"https://i.ibb.co/fFQtC15/2020-04-14-16-42-23.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J8JcoHtLT9J"
   },
   "source": [
    "### Forward pass\n",
    "\n",
    "Forward pass является *первым этапом итерации обучения нейросети*. После выполнения этого этапа сеть должна выдать вычисленное преобразование входа.\n",
    "\n",
    "Во время вызова метода `forward()` у `Sequential`, вход, поданный нейросети, проходит через все ее слои \"вперед\", до выходного слоя.\n",
    "\n",
    "Во время вызова метода `forward()` у `Module`, над входом, поданным слою, осуществляется операция этого слоя (линейная, дропаут, софтмакс, батчнорм).\n",
    "\n",
    "В реализации ниже у каждого слоя во время `forward()` будет вызываться только один метод $-$ `update_output()`, который и производит вычисление операции слоя. Важно отметить, что при вызове `update_output()` его выход **сохраняется в поле `self.output`** вызвавшего слоя. Это необходимо, поскольку выходы слоёв потом используются в **backward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4Z0Qyp-LT9L"
   },
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Теоретическая справка\n",
    "\n",
    "Backward pass является *вторым этапом итерации обучения нейросети*. В современном глубоком обучении backward pass является реализацией метода **[Error Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)** (backprop), по-русски **\"Метод обратного распространения ошибки\"**. После выполнения этого этапа у каждого параметра каждого слоя нейронной сети должны быть посчитаны градиенты на текущей итерации. \n",
    "\n",
    "**Первая идея** Backpropagation состоит в использования **градиентных методов оптимизации**, например, стохастического градиентного спуска. Однако чтобы посчитать градиент функции потерь $L$ по параметрам ранних слоев в нейросети, придется иметь дело с \"очень сложной\" функцией:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial (\\varphi_n(W_n\\varphi_{n-1}(W_{n-1}\\varphi_{n-2}(...\\varphi_{1}(W_1 x)))) - y)^2}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "где $W_k$ $-$ матрица весов $k$-го слоя, $\\varphi_k$ $-$ функция активации $k$-го слоя. И это при том, что здесь все слои $-$ линейные. Для более сложных слоев (например, свёрточных) эта функция будет еще сложнее.  \n",
    "\n",
    "Поэтому **вторая идея** Backpropagation состоит в использовании **правила цепочки (chain rule)**, примененного в отношении градиента функции потерь по каждому из параметров каждого слоя нейросети. Рассмотрим конкретный слой под номером $k$ и следующим за ним слой $k+1$. Пусть это оба $-$ линейные слои, линейный слой $k$ осуществляет операцию $O_k = x_k W_k$, где $x_k \\in \\mathbb{R}^{n \\times d}$ $-$ вход слоя, $W_k \\in \\mathbb{R}^{d \\times m}$ $-$ веса слоя, $O_k \\in \\mathbb{R}^{n \\times m}$ $-$ выход слоя. Тогда чтобы обновить его веса $W_k$ выпишем правило цепочки:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_k} = \\frac{\\partial L}{\\partial O_k} \\frac{\\partial O_k}{\\partial W_k} = \\frac{\\partial L}{\\partial O_k} x_k\n",
    "$$\n",
    "\n",
    "Видим, что для вычисления градиента лосса по $W_k$ нам нужно посчитать $\\frac{\\partial L}{\\partial O_k}$, то есть градиент лосса по выходу этого слоя. **Если слой $k$ является последним (выходным)** в нейросети (то есть $k+1$-го слоя уже нет), то ответ имеет вид:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial O_k} = \\frac{\\partial L}{\\partial \\widehat{y}_k}\n",
    "$$\n",
    "\n",
    "И сразу же достигаем цели. Однако **если слой $k$ $-$ это какой-то из скрытых слоев**, то мы не можем сразу посчитать $\\frac{\\partial L}{\\partial O_k}$ $-$ придем к той же проблеме \"очень сложной\" функции, описанной выше.  \n",
    "\n",
    "<img src=\"https://i.ibb.co/2hn6wb3/2020-04-14-17-19-29.png\" width=700>\n",
    "\n",
    "Поэтому чтобы реализовать правило цепочки, делается такой \"трюк\": заметим, что **выход слоя $k$ является входом для слоя $k+1$**, то есть $O_k = x_{k+1}$. И тогда будем для каждого слоя считать не только $\\frac{\\partial L}{\\partial W_{k+1}}$ для обновления весов, но и $\\frac{\\partial L}{\\partial x_{k+1}}$ для передачи градиента по входу слоя $k+1$ в виде градиента по выходу $\\frac{\\partial L}{\\partial O_k}$ слоя $k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{k+1}} = \\frac{\\partial L}{\\partial O_{k+1}} \\frac{\\partial O_{k+1}}{\\partial x_{k+1}} = \\frac{\\partial L}{\\partial O_{k+1}} W_{k+1}   \n",
    "$$\n",
    "\n",
    "Делая так для **каждого** слоя, мы получим возможность как бы **рекурсивно** обновлять параметры (веса) всех слоев, как только получим $\\frac{\\partial L}{\\partial \\hat{y_k}}$ от последнего слоя.\n",
    "\n",
    "#### Реализация\n",
    "\n",
    "Во время вызова метода `backward()` у `Sequential` мы в цикле вычисляем `backward()` для всех слоев нейросети в соответствии с описанной выше схемой реализации правила цепочки.\n",
    "\n",
    "Во время вызова метода `backward()` у `Module` вызываются два метода $-$ `update_grad_params()` и `update_grad_input()`.\n",
    "\n",
    "`update_grad_params()` вычисляет $\\frac{\\partial O_k}{\\partial W_k}$ $-$ градиент выхода слоя по параметрам $W_k$.\n",
    "\n",
    "`update_grad_input()` вычисляет $\\frac{\\partial O_k}{\\partial x_k}$ $-$ градиент выхода слоя по входу $x_k$, чтобы передать потом этот градент слою $k-1$ в виде `grad_output`.\n",
    "\n",
    "***Важно:*** в chain rule присутствуют произведения градиентов. Они могут быть векторами/матрицами, поэтому при умножении следует использовать именно **матричное произведение**, если выводите формулы через прозводную по вектору/матрице. Если же выводите \"поэлементно\" (как в примере с `LogSoftMax`), то форма произведений будет видна из вывода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwyYLB9ILT9N"
   },
   "source": [
    "Обратите внимание на то, что в цикле обучения выше (под картинкой в начале раздела \"Мотивация\") `last_grad_input` $-$ это градиент слоя `criterion` по его входу, и он же является `grad_output` для всей нейросети `model` $-$ градиентом, приходящим от \"следующего слоя\". Это полностью согласуется с методом обратного распространения ошибки, который мы только что обсудили, если считать функцию потерь (`criterion`) \"фиктивным\" слоем нейросети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39UTTy1oLT9N"
   },
   "source": [
    "*Примечание*: вообще говоря, сам метод обновления весов нейросети не обязан быть gradient-based, каким является backprop. Например, это могут быть [эволюционные алгоритмы](https://arxiv.org/pdf/1712.06567.pdf), или относительно недавний Equilibrium propagation, см. [ответ на StackOverflow](https://stackoverflow.com/questions/55287004/are-there-alternatives-to-backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2E-JGfRLT9O"
   },
   "source": [
    "### Обновление весов\n",
    "\n",
    "Обновление весов (оптимизация) является *третьим, последним этапом итерации обучения нейросети*. После выполнения этого этапа все обучаемые параметры всех слоев нейросети должны изменить свое значение (обновиться) в соответствие с правилами данного конкретного оптимизатора.\n",
    "\n",
    "В реализации ниже вам нужно написать только один оптимизатор $-$ `SGD`. Это метод стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3llAxwxLT9O"
   },
   "source": [
    "## Реализация (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edyuzHU7LT9O"
   },
   "source": [
    "<img src=\"https://i.insider.com/5cdc1519021b4c73922760ab?width=1100&format=jpeg&auto=webp\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR6CbroLLT9P"
   },
   "source": [
    "Далее вам предстоит реализовать все компоненты нейронной сети, используя **только библиотеку NumPy**:\n",
    "\n",
    "> Базовые концепции:\n",
    "- [x] `Module`     $-$ абстрактный класс для компонент нейронной сети;\n",
    "- [x] `Sequential` $-$ класс, содержащий в себе последовательность объектов класса `Module`.\n",
    "\n",
    "> Слои:\n",
    "- [x] `Linear`     $-$ линейный слой;\n",
    "- [x] `SoftMax`    $-$ слой, вычисляющий операцию *softmax*;\n",
    "- [x] `LogSoftMax` $-$ слой, вычисляющий операцию *log(softmax)*;\n",
    "- [ ] *(2 балла)* `Dropout` &mdash; слой дропаута;\n",
    "- [ ] *(3 балла)* `BatchNormalization` &mdash; слой для работы с батч-нормализацией;\n",
    "- [ ] *(1 балл)* `Scaling` &mdash; слой для работы с батч-нормализацией;\n",
    "- [ ] *(1 балл)* `Flatten` &mdash; слой, который просто разворачивает тензор любой размерности в одномерный вектор.\n",
    "\n",
    "> Функции активации (тоже являются слоями, но выделены в отдельную секцию для удобства):\n",
    "- [x] `ReLU`      $-$ функция активации *Rectified Linear Unit*;\n",
    "- [ ] *(1 балл)* `LeakyReLU` &mdash;  функция активации Leaky Rectified Linear Unit;\n",
    "- [ ] *(1 балл)* `ELU` &mdash; функция активации *Exponential Linear Unit*;\n",
    "\n",
    "> Функции потерь:\n",
    "- [x] `Criterion`  $-$ абстрактный класс для функций потерь;\n",
    "- [x] `NLLCriterionUnstable` $-$ negative log-likelihood функция потерь (нестабильная версия, возможны числовые переполнения);\n",
    "- [x] `NLLCriterion` $-$ negative log-likelihood функция потерь (стабильная версия).\n",
    "\n",
    "> Оптимизаторы:\n",
    "- [x] `SGD`  $-$ алгоритм стохастического градиентного спуска.\n",
    "- [ ] *(1 балл)* &mdash; любой другой оптимизатор, который мы разбирали на лекции.\n",
    "\n",
    "Перед каждым слоем напоминается формула его forward pass. В уже реализованных за вас модулях (отмечены галочкой) формулы для вычисления backward pass тоже уже даны, в остальных их нужно вывести самим по аналогии.\n",
    "\n",
    "**В скобках перед названием слоя указаны баллы за его реализацию и за вывод формулы для backward pass. Они засчитываются только тогда, когда слой проходит все тесты в ноутбуке `[2]task8_test_modules.ipynb`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I9zljPBjLT9Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twkdR5KsLT9R"
   },
   "source": [
    "##  Базовые концепции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe2EzNqBLT9S"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4ee2hw1LT9S"
   },
   "source": [
    "**Module** $-$ абстрактный класс, который определяет методы, которые могут быть реализованы у каждого слоя.\n",
    "\n",
    "Этот класс полностью реализован за вас. Пожалуйста, внимательно прочитайте методы и их описания, чтобы ориентироваться в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3smvdgDHLT9S"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "        Абстрактный класс для слоев нейросети.\n",
    "\n",
    "        Как и описано в \"Проектирование фреймворка\":\n",
    "\n",
    "        - во время forward просто вычисляет операцию слоя:\n",
    "\n",
    "            `output = module.forward(input)`\n",
    "\n",
    "        - во время backward дифференцирует функцию слоя по входу и по параметрам,\n",
    "          возвращает градиент по входу этого слоя (для удобства):\n",
    "\n",
    "            `grad_input = module.backward(input, grad_output)`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Вычисляет операцию слоя.\n",
    "        \n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя  \n",
    "        Выход: \n",
    "            `self.update_output(input) (np.array)` -- вычисленная операция слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.update_output(input)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Осуществляет шаг backpropagation'а для этого слоя,\n",
    "        дифференцируя функцию слоя по входу и по параметрам.\n",
    "        \n",
    "        Обратите внимание, что градиент зависит и от параметров, от входа input.\n",
    "        \n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        Выход: \n",
    "            `self.grad_input (np.array)` -- градиент функции слоя по входу\n",
    "        \"\"\"\n",
    "        \n",
    "        self.update_grad_input(input, grad_output)\n",
    "        self.update_grad_params(input, grad_output)\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        \"\"\"\n",
    "        Конкретная реализация `forward()` для данного слоя.\n",
    "        Вычисляет функцию слоя (линейную, `ReLU`, `SoftMax`) по входу `input`.\n",
    "        \n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя\n",
    "        Выход: \n",
    "            `self.output (np.array)` -- вычисленная операция слоя, сохраненная в поле класса \n",
    "            \n",
    "        Важно! не забывайте как возвращать `self.output`, так и сохранять результат в это поле \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Вычисляет градиент функции слоя по входу `input` и возвращает его в виде `self.grad_input`.\n",
    "        Размер (`shape`) поля `self.grad_input` всегда совпадает с размером `input`.\n",
    "        \n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        Выход: \n",
    "            `self.grad_input (np.array)` -- вычисленный градиент функции слоя по входу `input`\n",
    "        \n",
    "        Важно! не забывайте как возвращать `self.grad_input`, так и сохранять результат в это поле \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.grad_input = grad_output \n",
    "        # return self.grad_input\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def update_grad_params(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Вычисляет градиент функции слоя по параметрам (весам) этого слоя. \n",
    "        Ничего не возвращает, только сохраняет значения градиентов в соответствующие поля.\n",
    "        Не нужно реализовывать этот метод, если у слоя нет параметров (у функций активации, \n",
    "        `SoftMax`, `LogSoftMax`, `MaxPool2d`).\n",
    "        \n",
    "        Вход: \n",
    "           `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def zero_grad_params(self): \n",
    "        \"\"\"\n",
    "        Обнуляет градиенты у параметров слоя (если они есть).\n",
    "        Нужно для оптимизатора.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список параметров этого слоя, если они есть. Иначе вернуть пустой список. \n",
    "        Нужно для оптимизатора.\n",
    "        \"\"\"\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def get_grad_params(self):\n",
    "        \"\"\"\n",
    "        Возвращает список градиентов функции этого слоя по параметрам этого слоя, если они есть. \n",
    "        Иначе вернуть пустой список. \n",
    "        Нужно для оптимизатора.\n",
    "        \"\"\"\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Переключить слой в режим обучения.\n",
    "        От этого зависит поведение слоев `Dropout` и `BatchNorm`.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Переключить слой в режим тестирования.\n",
    "        От этого зависит поведение слоев `Dropout` и `BatchNorm`.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.training = False\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Напечатать название слоя КРАСИВО.\n",
    "        \"\"\"\n",
    "        \n",
    "        return 'Module'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dTkEMXhLT9U"
   },
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pttxoLWFLT9U"
   },
   "source": [
    "Многослойная нейронная сеть состоит из последовательности модулей. Реализуйте класс **Sequential**, руководствуюясь механикой forward и backward pass'ов и описаниями каждого метода.\n",
    "\n",
    "**Важно**: Убедитесь, что в `backward()` подаете на вход каждому слою НЕ `input` к этому `backward`'у нейросети,\n",
    "а именно тот вход, который слой `i` получал на соответствующей итерации `forward`'а (см. `update_output`).\n",
    "То есть что вход слоя `i` $-$ это выход слоя `self.modules[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e4qKhqmQLT9U"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "        Этот класс является последовательностью модулей (слоев). \n",
    "        Последовательно обрабатывает вход `input` от слоя к слою.\n",
    "        \n",
    "        Обратите внимание, он тоже наследуется от `Module`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Добавляет модуль в контейнер.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.modules.append(module)\n",
    "\n",
    "    def update_output(self, input):\n",
    "        \"\"\"\n",
    "        Соответствуя разделу \"Проектирование фреймворка\":\n",
    "        \n",
    "            O_0    = module[0].forward(input)\n",
    "            O_1    = module[1].forward(O_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(O_{n-2})   \n",
    "             \n",
    "        Нужно просто написать соответствующий цикл. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.output = [input]\n",
    "        for i in range(len(self.modules)):\n",
    "            self.output.append(self.modules[i].forward(self.output[-1]))\n",
    "        return self.output[-1]\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Соответствуя разделу \"Проектирование фреймворка\":\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(O_{n-2}, grad_output)\n",
    "            g_{n-2} = module[n-2].backward(O_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(O_0, g_2)   \n",
    "            grad_input = module[0].backward(input, g_1)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.grad_input = [grad_output]\n",
    "        for i in range(len(self.modules)-1, -1, -1):\n",
    "            self.grad_input.append(self.modules[i].backward(self.output[i], self.grad_input[-1]))\n",
    "        return self.grad_input[-1]\n",
    "      \n",
    "\n",
    "    def zero_grad_params(self): \n",
    "        for module in self.modules:\n",
    "            module.zero_grad_params()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Собирает параметры каждого слоя в один список, получая список списков.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [x.get_parameters() for x in self.modules]\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        \"\"\"\n",
    "        Собирает градиенты параметров каждого слоя в один список, получая список списков.\n",
    "        \"\"\"\n",
    "        \n",
    "        return [x.get_grad_params() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoqSNndiLT9V"
   },
   "source": [
    "## Слои"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2YZQwg6LT9V"
   },
   "source": [
    "### Linear\n",
    "\n",
    "Линейный слой, также известный как `Fully-Connected (FC)` или `Dense`, осуществляет линейное (афинное) преобразование.\n",
    "\n",
    "Везде ниже $N$ - размер батча, $d$ - число признаков во входном тензоре, $K$ - количество нейронов в слое.\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times d}, W \\in \\mathbb{R}^{d \\times K}, b \\in \\mathbb{R}^{1 \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Linear}(x) = x W + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Linear}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "*Backward pass (1 балл):*\n",
    "\n",
    "Могут помочь [эта ссылка](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf), [эта ссылка](http://cs231n.stanford.edu/vecDerivs.pdf) и [эта сслыка](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf).\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Linear}}{\\partial x} = W^T,~~~\\frac{\\partial \\text{Linear}}{\\partial W} = x,~~~\\frac{\\partial \\text{Linear}}{\\partial b} = \\vec{1}\n",
    "$$\n",
    "\n",
    "где $\\vec{1}$ $-$ вектор-строка из единиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "7gtLkuVBLT9V"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Слой, осуществляющий линейное преобразование\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        '''\n",
    "        Поля:\n",
    "            W - матрица весов слоя размера (n_in, n_out); \n",
    "                в данном случае n_in равно числу признаков, \n",
    "                а n_out равно количеству нейронов в слое\n",
    "            b - вектор свободных членов, по одному числу на один нейрон\n",
    "            gradW - хранит градиент матрицы весов линейного слоя\n",
    "            gradb - хранит градиент вектора свободных членов\n",
    "        '''\n",
    "        \n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_in, n_out))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def update_output(self, input):\n",
    "        self.output = input @ self.W + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        self.grad_input = grad_output @ self.W.T\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_grad_params(self, input, grad_output):\n",
    "        grad_output = np.array(grad_output)\n",
    "        self.gradW = input.T @ grad_output\n",
    "        self.gradb = grad_output.sum(axis=0)  # grad_output @ np.ones(self.b.shape[0])[:,None]\n",
    "        assert self.gradb.shape == self.b.shape\n",
    "    \n",
    "    def zero_grad_params(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = f'Linear {s[0]} -> {s[1]}'\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I16cFZoJLT9W"
   },
   "source": [
    "### SoftMax\n",
    "\n",
    "SoftMax слой осуществляет softmax-преобразование: \n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(x)_i = \\frac{\\exp(x_i)} {\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "Обозначим `batch_size` = $N$, `n_in` = $K$.\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "Тогда для батча SoftMax записывается так:\n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(x) = \\begin{pmatrix}\n",
    "\\frac{e^{x_{11}}} {\\sum_{j=1}^K e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j=1}^K e^{x_{1j}}} & \\dots & \\frac{e^{x_{1K}}} {\\sum_{j=1}^K e^{x_{1j}}} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\frac{e^{x_{N1}}} {\\sum_{j=1}^K e^{x_{Nj}}} & \\frac{e^{x_{N2}}} {\\sum_{j=1}^K e^{x_{Nj}}} & \\dots & \\frac{e^{x_{NK}}} {\\sum_{j=1}^K e^{x_{Nj}}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3O1bA8LLT9W"
   },
   "source": [
    "*Backward pass (2 балла):*\n",
    "\n",
    "На самом деле все не так тривиально, как может показаться. SoftMax не имеет параметров, но применяется ко входу поэлементно, поэтому дифференцируя выход этого слоя по входу мы получаем не градиент (=вектор производных), а якобиан (=матрицу производных). Пусть $x$ сейчас $-$ это **один вектор-строка из батча**, имеющая длину $K$. \n",
    "\n",
    "#### Якобиан SoftMax по входу:\n",
    "\n",
    "Помним, что:\n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(x) = \\begin{pmatrix}\n",
    "\\frac{e^{x_1}} {\\sum_{j=1}^K e^{x_j}} & \\frac{e^{x_2}} {\\sum_{j=1}^K e^{x_j}} & \\dots & \\frac{e^{x_K}} {\\sum_{j=1}^K e^{x_j}}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "a_1 & a_2 & \\dots & a_K\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$-$ обозначали за $a$ для удобства. Тогда:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\text{SoftMax}}{\\partial x} = \\begin{pmatrix}\n",
    "\\frac{\\partial a_1}{\\partial x_1} & \\frac{\\partial a_1}{\\partial x_2} & \\dots & \\frac{\\partial a_1}{\\partial x_K} \\\\\n",
    "\\frac{\\partial a_2}{\\partial x_1} & \\frac{\\partial a_2}{\\partial x_2} & \\dots & \\frac{\\partial a_2}{\\partial x_K} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\frac{\\partial a_K}{\\partial x_1} & \\frac{\\partial a_K}{\\partial x_2} & \\dots & \\frac{\\partial a_K}{\\partial x_K} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Распишем один элемент этой матрицы и поймем, какой конкретно вид он имеет. Возьмем частную производную $a_k$ по $x_s$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_k}{\\partial x_s} = \\frac{\\partial \\left( \\frac{e^{x_k}} {\\sum_{j=1}^K e^{x_j}}\\right)}{\\partial x_s} = \\frac{\\partial e^{x_k}}{\\partial x_s} \\left( \\frac{1}{\\sum_{j=1}^K e^{x_j}}\\right) + e^{x_k} \\frac{\\partial \\left(\\frac{1}{\\sum_{j=1}^K e^{x_j}}\\right)}{\\partial x_s} = \\\\\n",
    "= \\frac{\\partial e^{x_k}}{\\partial x_s} \\left( \\frac{1}{\\sum_{j=1}^K e^{x_j}}\\right) - e^{x_k} \\left( \\frac{1}{\\left(\\sum_{j=1}^K e^{x_j} \\right)^2} \\right) e^{x_s} \\tag{1}\n",
    "$$\n",
    "\n",
    "Далее все зависит от $k$ и $s$. Если $k = s$, то первое слагаемое в (1) не зануляется и мы получаем:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_k}{\\partial x_k} = \\frac{\\partial e^{x_k}}{\\partial x_k} \\left( \\frac{1}{\\sum_{j=1}^K e^{x_j}}\\right) - e^{x_k} \\left( \\frac{1}{\\left(\\sum_{j=1}^K e^{x_j} \\right)^2} \\right) e^{x_k} = \\frac{e^{x_k}}{\\sum_{j=1}^K e^{x_j}} \\left( 1 - \\frac{e^{x_k}}{\\sum_{j=1}^K e^{x_j}} \\right) = a_k (1 - a_k)\n",
    "$$\n",
    "\n",
    "Если же $k \\ne s$, то первое слагаемое в (1) обнулится:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_k}{\\partial x_s} = - e^{x_k} \\left( \\frac{1}{\\left(\\sum_{j=1}^K e^{x_j} \\right)^2} \\right) e^{x_s} = - a_k a_s\n",
    "$$\n",
    "\n",
    "Таким образом для **одной строки в батче** получаем:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\text{SoftMax}}{\\partial x} = \\begin{pmatrix}\n",
    "a_1 (1 - a_1) & -a_1 a_2 & \\dots & -a_1 a_K \\\\\n",
    "-a_2 a_1 & a_2 (1 - a_2) & \\dots & -a_2 a_K \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "-a_K a_1 & -a_K a_2 & \\dots & a_K (1 - a_K) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Вывод `grad_input`\n",
    "\n",
    "Теперь нужно понять, как именно записать `grad_input` в контексте backprop'а. Нам приходит от следующего слоя `grad_output` того же размера, что и его вход: $(N, K)$, где $N$ =`batch_size`, $K$ = `n_in`. Следующий после SoftMax \"слой\" не обязан быть функцией потерь (хотя на практике обычно это именно так), но все же обозначим для удобства функцию следующего слоя как $L$. \n",
    "\n",
    "Сейчас снова считаем, что $x$ $-$ вектор-строка в матрице всего батча, `grad_output` сейчас тоже является строкой $(1, K)$ в матрице всего `grad_output`. По [формуле полного дифференциала](https://mipt.ru/education/chair/mathematics/study/uchebniki/IvanovGE_part1.pdf) имеем для $x_s$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_s} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_s} = \\frac{\\partial L}{\\partial a_s} \\frac{\\partial a_s}{\\partial x_s} + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_s} = \\frac{\\partial L}{\\partial a_s} a_s (1 - a_s) + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial a_i} (-a_i a_s) = \\\\\n",
    "= \\frac{\\partial L}{\\partial a_s} a_s  - \\frac{\\partial L}{\\partial a_s} a_s^2 - a_s \\sum_{i\\ne s} \\frac{\\partial L}{\\partial a_i} a_i = \\frac{\\partial L}{\\partial a_s} a_s - a_s \\sum_{i=1}^K \\frac{\\partial L}{\\partial a_i} a_i\n",
    "$$\n",
    "\n",
    "Здесь $\\frac{\\partial L}{\\partial a_i}$ является $i$-ым элементом вектор-строки `grad_output`, которая соответствует рассматриваемой вектор-строке $x$. То есть мы получили вывод `grad_input` по `grad_output` и `SoftMax(x)`для **одной вектор-строки из батча**.\n",
    "\n",
    "Если присмотеться к формуле и выписать `grad_output` уже для батча (номер строки в батче обозначен верхним индексом):\n",
    "\n",
    "$$\n",
    "\\text{grad_output} = \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial a_1}^1 & \\frac{\\partial L}{\\partial a_2}^1 & \\dots & \\frac{\\partial L}{\\partial a_K}^1 \\\\\n",
    "\\frac{\\partial L}{\\partial a_1}^2 & \\frac{\\partial L}{\\partial a_2}^2 & \\dots & \\frac{\\partial L}{\\partial a_K}^2 \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\frac{\\partial L}{\\partial a_1}^N & \\frac{\\partial L}{\\partial a_2}^N & \\dots & \\frac{\\partial L}{\\partial a_K}^N \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "И помнить, что :\n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(x) = \\begin{pmatrix}\n",
    "a_1^1 & a_2^1 & \\dots & a_K^1 \\\\\n",
    "a_1^2 & a_2^2 & \\dots & a_K^2 \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_1^N & a_2^N & \\dots & a_K^N \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$-$ легко записать формулу для `grad_input` в матричной форме, что и есть выход метода `update_grad_input`.\n",
    "\n",
    "Полный вывод также есть [здесь](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) и [здесь](http://hiroshiu.blogspot.com/2018/10/gradient-of-softmax-function.html).\n",
    "\n",
    "*Подсказка:* В коде используйте свойство: $\\text{softmax}(x) = \\text{softmax}(x - \\text{const})$. Это позволяет избежать переполнения при вычислении экспоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jB1ZZaltLT9X"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "\n",
    "    def update_output(self, input):\n",
    "        # нормализуем для численной устойчивости\n",
    "        self.output = input - input.max(axis=1, keepdims=True)\n",
    "        self.output = np.exp(self.output)\n",
    "        self.output = self.output / (np.sum(self.output, axis=1)).reshape(-1, 1)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        input_clamp = input - input.max(axis=1, keepdims=True)\n",
    "        output = np.exp(input_clamp)\n",
    "        output = output / np.sum(output, axis=1).reshape(-1, 1)\n",
    "\n",
    "        self.grad_input = grad_output * output\n",
    "        self.grad_input -= output * np.sum(grad_output * output, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'SoftMax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Jw9P_bTLT9Y"
   },
   "source": [
    "### LogSoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN1HSCM0LT9Y"
   },
   "source": [
    "LogSoftMax слой есть просто логарифм от softmax-преобразования: \n",
    "\n",
    "$$\n",
    "\\text{logsoftmax}(x)_i = \\log(\\text{softmax}(x))_i = x_i - \\log {\\sum_j \\exp x_j}\n",
    "$$\n",
    "\n",
    "По полной аналогии с LogSoftMax-слоем распишем forward и backward:  \n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "Обозначим `batch_size` = $N$, `n_in` = $K$.\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "Тогда для батча LogSoftMax записывается так:\n",
    "\n",
    "$$\n",
    "\\text{LogSoftMax}(x) = \\begin{pmatrix}\n",
    "x_{11} - \\log {\\sum_{j=1}^K e^{x_j}} & x_{12} - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_{1K} - \\log {\\sum_{j=1}^K e^{x_j}} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_{N1} - \\log {\\sum_{j=1}^K e^{x_j}} & x_{N2} - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_{NK} - \\log {\\sum_{j=1}^K e^{x_j}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LogSoftMax}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh80sUfNLT9Y"
   },
   "source": [
    "*Backward pass:*\n",
    "\n",
    "LogSoftMax не имеет параметров, но применяется ко входу поэлементно, поэтому дифференцируя выход этого слоя по входу мы получаем не градиент (=вектор производных), а якобиан (=матрицу производных). Пусть $x$ сейчас $-$ это **один вектор-строка из батча**, имеющая длину $K$. \n",
    "\n",
    "#### Якобиан LogSoftMax по входу:\n",
    "\n",
    "Помним, что:\n",
    "\n",
    "$$\n",
    "\\text{LogSoftMax}(x) = \\begin{pmatrix}\n",
    "x_1 - \\log {\\sum_{j=1}^K e^{x_j}} & x_2 - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_K - \\log {\\sum_{j=1}^K e^{x_j}}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "b_1 & b_2 & \\dots & b_K\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$-$ обозначали за $b$ для удобства. Тогда:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\text{LogSoftMax}}{\\partial x} = \\begin{pmatrix}\n",
    "\\frac{\\partial b_1}{\\partial x_1} & \\frac{\\partial b_1}{\\partial x_2} & \\dots & \\frac{\\partial b_1}{\\partial x_K} \\\\\n",
    "\\frac{\\partial b_2}{\\partial x_1} & \\frac{\\partial b_2}{\\partial x_2} & \\dots & \\frac{\\partial b_2}{\\partial x_K} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\frac{\\partial b_K}{\\partial x_1} & \\frac{\\partial b_K}{\\partial x_2} & \\dots & \\frac{\\partial b_K}{\\partial x_K} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Распишем один элемент этой матрицы и поймем, какой конкретно вид он имеет. Возьмем частную производную $b_k$ по $x_s$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_k}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_s} \\tag{1}\n",
    "$$\n",
    "\n",
    "Далее все зависит от $k$ и $s$. Если $k = s$, то первое слагаемое в (1) не зануляется и мы получаем:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_k}{\\partial x_k} = \\frac{\\partial x_k}{\\partial x_k} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = 1 - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_k} = 1 - a_k\n",
    "$$\n",
    "\n",
    "**Где $a_k$ $-$ это $k$-ая компонента SoftMax-слоя от этого входа**. Если же $k \\ne s$, то первое слагаемое в (1) обнулится:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_k}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_s} = a_s\n",
    "$$\n",
    "\n",
    "Таким образом для **одной строки в батче** получаем:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\text{LogSoftMax}}{\\partial x} = \\begin{pmatrix}\n",
    "(1 - a_1) & -a_2 & \\dots & -a_K \\\\\n",
    "-a_1 & (1 - a_2) & \\dots & -a_K \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "-a_1 & -a_2 & \\dots & (1 - a_K) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Вывод `grad_input`:\n",
    "\n",
    "Полностью аналогично расписанному выше для SoftMax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_s} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial b_i}\\frac{\\partial b_i}{\\partial x_s} = \\frac{\\partial L}{\\partial b_s} \\frac{\\partial b_s}{\\partial x_s} + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i}\\frac{\\partial b_i}{\\partial x_s} = \\frac{\\partial L}{\\partial b_s} (1 - a_s) + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i} (-a_s) = \n",
    "\\frac{\\partial L}{\\partial b_s} - a_s \\frac{\\partial L}{\\partial b_s} - a_s \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial b_s} - a_s \\sum_{i=1}^K \\frac{\\partial L}{\\partial b_i}\n",
    "$$\n",
    "\n",
    "Теперь легко записать формулу для `grad_input` в матричной форме, что и есть выход метода `update_grad_input()`.\n",
    "\n",
    "*Подсказка:* В коде используйте свойство: $\\text{logsoftmax}(x) = \\text{logsoftmax}(x - \\text{const})$. Это позволяет избежать переполнения при вычислении экспоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rwNWM9UNLT9Z"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        # нормализуем для численной устойчивости\n",
    "        self.output = input - input.max(axis=1, keepdims=True)\n",
    "        self.output = self.output - np.log(np.sum(np.exp(self.output), axis=1)).reshape(-1, 1)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        input_clamp = input - input.max(axis=1, keepdims=True)\n",
    "        output = np.exp(input_clamp)\n",
    "        output = output / np.sum(output, axis=1).reshape(-1, 1)\n",
    "\n",
    "        self.grad_input = grad_output\n",
    "        self.grad_input -= output * np.sum(grad_output, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return self.grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'LogSoftMax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdXwfbHe7Ap2"
   },
   "source": [
    "### Dropout (2 балл = 1 [формула] + 1 [код])\n",
    "\n",
    "[**Dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) слой просто \"отключает\" (зануляет) нейроны слоя, после котрого он стоит, с некоторой вероятностью $p$. Реализуется это тоже весьма просто: умножаем маску из независимых случайных величин, имеющих распределение $Bern(p)$ на выход предыдущего слоя. \n",
    "\n",
    "На практике установлено, что этот слой помогает бороться с переобучением и не дает нейронам \"привыкнуть\" к конкретным выходам предыдущего слоя, что тоже может привести к переобучению.\n",
    "\n",
    "В фазе обучения (`self.training == True`) нужно семплировать маску для каждого батча по-отдельности, зануляя некоторые их входы, и деля результат на $1 / (1 - p)$. Умножение на $1 / (1 - p)$ нужно для того, чтобы средние значения признаков были теми же, что будут в тесте. \n",
    "\n",
    "В фазе тестирования дропаут отключают, то есть слой становится просто тождественным преобразованием: `self.output = input`.\n",
    "\n",
    "*Forward pass:*\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "$$\n",
    "M \\in \\{0,1\\}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Dropout}(x) = x \\cdot M \\cdot \\frac{1}{1 - p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Dropout}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "Где $\\cdot$ $-$ поэлементное умножение.\n",
    "\n",
    "*Backward pass (1 балл):*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Dropout}}{\\partial x} = M \\cdot \\frac{1}{1 - p}\n",
    "$$\n",
    "\n",
    "Параметров у слоя нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Cf1C3_Hj7ECS"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = []\n",
    "        \n",
    "    def update_output(self, input):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.p, input.shape[0] * input.shape[1]).reshape((input.shape[0], \n",
    "                                                                                                input.shape[1]))\n",
    "            self.output = input * self.mask / (1 - self.p)\n",
    "        \n",
    "        else:\n",
    "            self.output = input\n",
    "            \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            self.grad_input = grad_output * self.mask / (1 - self.p)\n",
    "            \n",
    "        else:\n",
    "            self.grad_input = grad_output\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.grad_input\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Dropout'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuwSVi5d77SY"
   },
   "source": [
    "### Batch normalization (4 балла = 3 BatchNormalization [код] + 1 Scaling [код], формулы есть по ссылке)\n",
    "\n",
    "[**Batch Normalization (BN)**](http://arxiv.org/abs/1502.03167) $-$ идея batch normalization на самом деле содержится в самом названии $-$ будем нормализовать выход каждого слоя: вычитать из значения каждого признака среднее его значение по текущему батчу и делить на стандартное отклонение. Такой процесс является частным случаем **[whitening](https://en.wikipedia.org/wiki/Whitening_transformation)** ($mean = 0$, $std = 1$). После этого значения признаков умножаются на обучемый параметр `gamma` и прибавляется обучаемый свободный член `beta`, что позволяет \"контролировать\" среднее значение и дисперсию признаков.\n",
    "\n",
    "На практике BatchNorm обычно ускоряет сходимость при оптимизации, то есть позволяет обучать нейросети значительно быстрее. Вам нужно реализовать только первую часть этого слоя, которая нормализует вход. `Scaling` слой, в котором результат умножается на `gamma` и складывается с `beta`, уже реализован.\n",
    "\n",
    "То есть в данной реализации единый по своей сути слой Batch Normalization разбит на два этапа (слоя):\n",
    "1. `BatchNormalization`: вычитание `mean` и деление на `std`\n",
    "2. `Scaling`: умножение на `gamma` и прибавление `beta`\n",
    "\n",
    "**BatchNormalization**\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "$$\n",
    "\\mu \\in \\mathbb{R}^{1 \\times K}\n",
    "$$\n",
    "$$\n",
    "\\sigma \\in \\mathbb{R}^{1 \\times K}\n",
    "$$\n",
    "\n",
    "В фазе обучения  (`self.training == True`) BatchNormalization слой делает то, что описано выше:\n",
    "\n",
    "$$\n",
    "\\text{BatchNormalization}(x) = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{BatchNormalization}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "где $\\mu$ и $\\sigma$ $-$ среднее и дисперсия значений признаков в $x$ ($\\varepsilon$ нужен, чтобы избежать деление на машинный 0). Также в фазе обучения среднее и дисперсию признаков следует обновлять (moving average): \n",
    "\n",
    "$$\n",
    "\\mu = \\alpha \\mu + \\widehat{\\mu} (1 - \\alpha)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\alpha \\sigma + \\widehat{\\sigma} (1 - \\alpha)\n",
    "$$\n",
    "\n",
    "где $\\widehat{\\mu}, \\widehat{\\sigma}$ $-$ среднее и дисперсия по текущему батчу.\n",
    "\n",
    "В фазе тестирования (`self.training == False`) слой нормализует вход `input`, используя посчитанные в фазе обучения `moving_mean` и `moving_variance`. \n",
    "\n",
    "*Backward pass:*\n",
    "\n",
    "В [оригинальной статье](https://arxiv.org/pdf/1502.03167.pdf) на странице 4 есть все формулы для реализации backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ziP-as6w8Crs"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "    \n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = None \n",
    "        self.moving_variance = None\n",
    "        \n",
    "    def update_output(self, input):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            self.batch_mean = np.mean(input, axis=0)\n",
    "            self.batch_variance = np.var(input, axis=0)\n",
    "            if self.moving_mean is not None:\n",
    "                self.moving_mean = self.alpha * self.moving_mean + (1 - self.alpha) * self.batch_mean\n",
    "                self.moving_variance = self.alpha * self.moving_variance + (1 - self.alpha) * self.batch_variance\n",
    "            else:\n",
    "                self.moving_mean = self.batch_mean\n",
    "                self.moving_variance = self.batch_variance\n",
    "                \n",
    "        else:\n",
    "            self.batch_mean = self.moving_mean\n",
    "            self.batch_variance = self.moving_variance\n",
    "                \n",
    "        self.output = (input - self.batch_mean) / np.sqrt(self.batch_variance + self.EPS)  \n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_sigma = np.sum(grad_output * (input - self.batch_mean) * \\\n",
    "                            (-1 / 2 / ((self.batch_variance + self.EPS) ** 1.5)), axis=0)\n",
    "        grad_mu = np.sum(grad_output * (-1 / np.sqrt(self.batch_variance + self.EPS)), axis=0) + grad_sigma * \\\n",
    "                  np.sum(-2 * (input - self.batch_mean) / input.shape[0], axis=0)\n",
    "        self.grad_input = grad_output / np.sqrt(self.batch_variance + self.EPS) + \\\n",
    "                          grad_sigma * 2 * (input - self.batch_mean) / input.shape[0] + grad_mu / input.shape[0]\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'BatchNormalization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD3bwgHB8Ljj"
   },
   "source": [
    "**Scaling**\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "$$\n",
    "\\gamma \\in \\mathbb{R}^{1 \\times K}\n",
    "$$\n",
    "$$\n",
    "\\beta \\in \\mathbb{R}^{1 \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Scaling}(x) = \\gamma x + \\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Scaling}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "где $\\gamma$ и $\\beta$ $-$ обучаемые параметры слоя.\n",
    "\n",
    "*Backward pass:*\n",
    "\n",
    "В [оригинальной статье](https://arxiv.org/pdf/1502.03167.pdf) на странице 4 есть все формулы для реализации backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iIQFG3cL8GRN"
   },
   "outputs": [],
   "source": [
    "class Scaling(Module):\n",
    "    \n",
    "    def __init__(self, n_out):\n",
    "        super(Scaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=(1,n_out))\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=(1,n_out))\n",
    "        \n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def update_output(self, input):\n",
    "        self.output = self.gamma * input + self.beta\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "\n",
    "        self.grad_input = self.gamma * grad_output\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_grad_params(self, input, grad_output):\n",
    "        \n",
    "        self.gradBeta = np.sum(grad_output, axis=0) # <ВАШ КОД ЗДЕСЬ>\n",
    "        self.gradGamma = np.sum(grad_output * input, axis=0) # <ВАШ КОД ЗДЕСЬ>\n",
    "    \n",
    "    def zero_grad_params(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Scaling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrYJWbzG8VE5"
   },
   "source": [
    "*Примечание:* BatchNormalization $-$ не единственный вид нормализации в Deep Learning. См. [обзор normalization слоев](https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaDfJpeR8Zn8"
   },
   "source": [
    "### Flatten (1 балл = 1[код])\n",
    "\n",
    "`Flatten` нужен для того, чтобы тензор размера `(batch_size, d_1, ..., d_n)` преобразовать в тензор размера `(batch_size, d_1 * ... * d_n)` в forward pass. Заметим, что при обратном проходе градиенты, которые подаются на вход имеют размер `(batch_size, d_1 * ... * d_n)`, и должны быть преобразованы обратно в `(batch_size, d_1, ..., d_n)`. `Flatten` может пригодится нам при работе с датасетом FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PvQAx1i78bAa"
   },
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    def __init__(self):\n",
    "         super(Flatten, self).__init__()\n",
    "    \n",
    "    def update_output(self, input):\n",
    "\n",
    "        self.output = input.reshape(np.prod(input.shape), -1)\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "      \n",
    "        self.grad_input = grad_output.reshape(input.shape)\n",
    "    \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Flatten'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du0Crhg-LT9c"
   },
   "source": [
    "## Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga5mNgXdLT9d"
   },
   "source": [
    "Функции активации $-$ это нелинейные функции, которые ставятся после `Linear`, `Conv` и других слоев. Именно благодаря им нейросети являются не просто одним большим линейным преобразованием, а сложной нелинейной функцией.\n",
    "\n",
    "Достаточно исчерпывающий список с описанием преимуществ и недостатков каждой из функций активации [см. здесь](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pBDuhNULT9d"
   },
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TJvlgs0LT9d"
   },
   "source": [
    "**[Rectified Linear Unit](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)** (**ReLU**) $-$ одна из самых часто используемых функций активации.\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "Применяется поэлементно.\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = max(0, x) = \\begin{cases}\n",
    "  0, & x \\le 0 \\\\    \n",
    "  x & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "*Backward pass:*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{ReLU}}{\\partial x} = \\begin{cases}\n",
    "  0, & x \\le 0 \\\\    \n",
    "  1 & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3tUW3UdHLT9d"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def update_output(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        self.grad_input = np.multiply(grad_output, input > 0)\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReLU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDNFCKuH8iTN"
   },
   "source": [
    "### Leaky ReLU (1 балл = 0.5 [формула] + 0.5 [код])\n",
    "\n",
    "**[Leaky Rectified Linear Unit](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)** (**LeakyReLU**) $-$ добавляет в ReLU контроль над зануляемой частью.\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "Применяется поэлементно.\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\begin{cases}\n",
    "  \\gamma x, & x \\le 0 \\\\    \n",
    "  x & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "*Backward pass (0.5 балла):*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{LeakyReLU}}{\\partial x} = \\begin{cases}\n",
    "  \\gamma, & x \\le 0 \\\\    \n",
    "  1 & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qD2vWkC18nmh"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def update_output(self, input):\n",
    "\n",
    "        self.output = np.maximum(input, 0) + self.slope * np.minimum(input, 0)\n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "\n",
    "        self.grad_input = ((input >= 0) + self.slope * (input < 0)) * grad_output\n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'LeakyReLU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rG7He7umFJGk"
   },
   "source": [
    "### ELU (1 балл = 0.5 [формула] + 0.5 [код])\n",
    "\n",
    "**[Exponential Linear Unit](http://arxiv.org/abs/1511.07289)** (**ELU**) $-$ другая форма контроля над зануляемой частью.\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "Применяется поэлементно.\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases}\n",
    "  a(e^x - 1), & x \\le 0 \\\\    \n",
    "  x & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "*Backward pass (0.5 балла):*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{ELU}}{\\partial x} = \\begin{cases}\n",
    "  a e^x, & x \\le 0 \\\\    \n",
    "  1 & x \\gt 0    \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "27MdSozg9fLr"
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def update_output(self, input):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        self.output = (input >= 0) * self.alpha * (np.exp(input) - 1) + input * (input < 0)\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Вход:\n",
    "            `input (np.array)` -- вход слоя\n",
    "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
    "        \"\"\"\n",
    "        \n",
    "        self.grad_input = grad_output * (self.alpha * np.exp(input) * (input >= 0) + (input < 0))\n",
    "        \n",
    "#         <ВАШ КОД ЗДЕСЬ>\n",
    "        \n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ELU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UVwpm8yLT9g"
   },
   "source": [
    "## Функции потерь (лосс, loss, criterion, objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK-0bBmFLT9g"
   },
   "source": [
    "*Примчание:* Формально это не функции потерь, а функции риска. Везде далее и в во всех наших материалах, связанными с нейросетями, следующие слова являются синонимами: \"лосс\", \"функция потерь\", \"loss\", \"criterion\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izE2BM5tLT9g"
   },
   "source": [
    "Функции потерь или лоссы (не путать с [мемом \"Loss\"](https://tjournal.ru/internet/68665-mem-loss)) являютя оптимизируемыми функциями в обучении с учителем. Если считать всю нейросеть одной большой функцией, то функцию потерь можно считать [функционалом](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB). \n",
    "\n",
    "Функции потерь не имеют параметров, а лишь вычисляют меру схожести ответов нейросети $\\widehat{y}$ (prediction) с истинными ответами $y$ (target, ground truth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRHXbbhyLT9g"
   },
   "source": [
    "### Criterion\n",
    "\n",
    "**Criterion** $-$ абстрактный класс функции потерь. Этот класс можно в целом считать последним слоем нейросети, однако для удобства этот класс не является наследником `Module`, а порождает автономное семейство классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeNMu0dULT9g"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь по входу `input` и истинными значениями `target`.\n",
    "\n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя  \n",
    "            `target (np.array)` -- истинные ответы\n",
    "        Выход: \n",
    "            `self.update_output(input, target) (np.array)` -- вычисленная функция потерь\n",
    "        \"\"\"\n",
    "        return self.update_output(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "        Вычисляет градиент функции потерь по входу `input`.\n",
    "        Использует для этого также истинные значения `target`.\n",
    "\n",
    "        Вход: \n",
    "            `input (np.array)` -- вход слоя  \n",
    "            `target (np.array)` -- истинные ответы\n",
    "        Выход: \n",
    "            `self.update_grad_input(input, target) (np.array)` -- вычисленный градиент по входу `input`\n",
    "        \"\"\"\n",
    "        return self.update_grad_input(input, target)\n",
    "    \n",
    "    def update_output(self, input, target):\n",
    "        \"\"\"\n",
    "        Фунция, реализующая `forward()`\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, target):\n",
    "        \"\"\"\n",
    "        Фунция, реализующая `backward()`\n",
    "        \"\"\"\n",
    "        return self.grad_input   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Напечатать название слоя КРАСИВО.\n",
    "        \"\"\"\n",
    "        return 'Criterion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnhPXvwtLT9i"
   },
   "source": [
    "### Negative LogLikelihood criterion (численно неустойчивый)\n",
    "**[NLLCriterion](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss)** $-$ является отрицанием логарифма функции правдоподобия (likelihood function), используется в задаче классификации. Является частным случаем [дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0). Принимает на вход истинные вероятности классов $y$ и предсказанные вероятности классов $\\widehat{y}$ от `SoftMax`-слоя.\n",
    "\n",
    "Истинные метки `y` на вход ожидаются уже **после One-Hot кодирования**.\n",
    "\n",
    "*Forward pass:*\n",
    "\n",
    "$$\n",
    "\\widehat{y} \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y \\in \\mathbb{R}^{N \\times K}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{NLLCriterion}(\\widehat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\widehat{y}_{ij})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{NLLCriterion}(\\widehat{y}, y) \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "*Backward pass (0.5 балла):*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{NLLCriterion}}{\\partial \\widehat{y}} = -\\frac{1}{N} \\frac{y}{\\widehat{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rs1G0vB7LT9i"
   },
   "outputs": [],
   "source": [
    "class NLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(NLLCriterionUnstable, self)\n",
    "        super(NLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "    def update_output(self, input, target): \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        self.output = -np.sum(np.multiply(target, np.log(input_clamp))) / input_clamp.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, target):\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        self.grad_input = -(target / input_clamp) / input_clamp.shape[0]\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'NLLCriterionUnstable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHHeHhwSLT9i"
   },
   "source": [
    "### Negative LogLikelihood criterion (численно устойчивый)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1alK0u1LT9i"
   },
   "source": [
    "Абсолютная копия `NLLCriterionUnstable` выше, но принимает на вход не `SoftMax`-вероятности, а выход `LogSoftMax` слоя. Подобная комбинация позволяет избежать проблем в этом слое с вычислениями `forward` и `backward` для логарифма.  \n",
    "\n",
    "Изменений немного, а формула наоборот упрощается. Поэтому напишем сразу код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sp5Hb6TMLT9i"
   },
   "outputs": [],
   "source": [
    "class NLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(NLLCriterion, self)\n",
    "        super(NLLCriterion, self).__init__()\n",
    "        \n",
    "    def update_output(self, input, target): \n",
    "        self.output = -np.sum(np.multiply(target, input)) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, input, target):\n",
    "        self.grad_input = -target / input.shape[0]\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'NLLCriterion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No1H2UY-LT9j"
   },
   "source": [
    "## Оптимизаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BcAJBx8LT9j"
   },
   "source": [
    "### SGD\n",
    "\n",
    "Оптимизатор, основанный на методе стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "3CpfEzxDLT9j"
   },
   "outputs": [],
   "source": [
    "def SGD(variables, gradients, config, state):  \n",
    "    '''\n",
    "    Реализация метода стохастического градиентого спуска.\n",
    "    Обновляет значения переменных в соответствии с их градиентами и сохраняет градиенты в state.\n",
    "    \n",
    "    Вход:\n",
    "        `variables` - список (`list`) списков переменных, которые нужно обновить \n",
    "         (один список для одного слоя)\n",
    "        `gradients` - список (`list`) списков градиентов этих переменных \n",
    "         (ровно та же структура, как и у `variables`, один список для одного слоя)\n",
    "        `config` - словарь (`dict`) c гиперпараметрами оптимизатора \n",
    "         (сейчас это только `learning_rate`)\n",
    "        `state` -  словарь (`dict`) c состоянием (`state`) оптимизатора\n",
    "    Выход:\n",
    "        Ничего не возвращает. Обновляет значения градиентов \n",
    "    '''\n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            current_var -= config['learning_rate'] * current_grad"
   ]
  },
  {
   "attachments": {
    "%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202022-03-18%20231653.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "i3aTtbdlGo4J"
   },
   "source": [
    "### Любой другой оптимизатор с лекции (1 балл = 1[реализация])\n",
    "\n",
    "Реализуем Адама. Воспользуемся следующими формулами с лекции:\n",
    "\n",
    "![%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202022-03-18%20231653.png](attachment:%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202022-03-18%20231653.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "cuMwWcOvG5qj"
   },
   "outputs": [],
   "source": [
    "def Adam(variables, gradients, config, state): \n",
    "    '''\n",
    "    Реализация метода Adam.\n",
    "    Обновляет значения переменных в соответствии с их градиентами и сохраняет градиенты в state.\n",
    "    \n",
    "    Вход:\n",
    "        `variables` - список (`list`) списков переменных, которые нужно обновить \n",
    "         (один список для одного слоя)\n",
    "        `gradients` - список (`list`) списков градиентов этих переменных \n",
    "         (ровно та же структура, как и у `variables`, один список для одного слоя)\n",
    "        `config` - словарь (`dict`) c гиперпараметрами оптимизатора \n",
    "         (`learning_rate`, `beta`, `mu`)\n",
    "        `state` -  словарь (`dict`) c состоянием (`state`) оптимизатора\n",
    "    Выход:\n",
    "        Ничего не возвращает. Обновляет значения градиентов \n",
    "    '''\n",
    "    EPS = 1e-15\n",
    "    state.setdefault('v', {})  # скорость\n",
    "    state.setdefault('g', {})  # нормализация\n",
    "    state.setdefault('t', 0)  # номер итерации\n",
    "    state['t'] += 1\n",
    "    \n",
    "    var_idx = 0\n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            velocity = state['v'].setdefault(var_idx, np.zeros_like(current_grad))\n",
    "            normalization = state['g'].setdefault(var_idx, np.zeros_like(current_grad))\n",
    "            velocity = config['beta'] * velocity + (1 - config['beta']) * current_grad\n",
    "            normalization = config['mu'] * normalization + (1 - config['mu']) * (current_grad ** 2)\n",
    "            current_var -= config['learning_rate'] / (np.sqrt(normalization + EPS) / \\\n",
    "                                    (1 - (config['mu'] ** state['t']))) * velocity / (1 - (config['beta'] ** state['t']))\n",
    "        \n",
    "        \n",
    "            var_idx += 1\n",
    "    \n",
    "#     <ВАШ КОД ЗДЕСЬ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIpCFWc5LT9k"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR53UZ-dLT9k"
   },
   "source": [
    "- Если хочется ускорить вычисления и написать действительно \"свой PyTorch\", можно использовать библиотеку [JAX](https://github.com/google/jax) от Google. Она является оберткой над [autograd](https://github.com/hips/autograd) (автоматическое дифференцирование) и [XLA](https://www.tensorflow.org/xla) (компиляция Python-кода)\n",
    "\n",
    "- До сих пор мы производили все вычисления на CPU. Однако Deep Learning расцвел благодаря GPU. Более конкретно $-$ благодаря [Nvidia GPU](https://developer.nvidia.com/cuda-gpus) и [Nvidia CUDA](https://developer.nvidia.com/cuda-zone). Они также очень активно используются для [компьютерной графики](https://en.wikipedia.org/wiki/Computer_graphics)\n",
    "\n",
    "- NumPy как раз можно запускать на GPU: раньше для этого чаще использовали [Numba](https://github.com/numba/numba), однако сейчас (в 2020 году) есть [много удобных библиотек для этого](https://stsievert.com/blog/2016/07/01/numpy-gpu/)\n",
    "\n",
    "- Конечно же, вы всегда можете просто использовать PyTorch для работы с GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BPXq0XfLT9k"
   },
   "source": [
    "<img src=\"https://pwnews.net/_fr/108/8422266.png\" width=500>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[1.1]_modules.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
